[{"title": "Begin Document", "subsections": [{"title": "", "content": "\nWe evaluate the downstream task performance of LoRA on RoBERTa~, DeBERTa~, and GPT-2~, before scaling up to GPT-3 175B~.\nOur experiments cover a wide range of tasks, from natural language understanding (NLU) to generation (NLG).\nSpecifically, we evaluate on the GLUE~ benchmark for RoBERTa and DeBERTa.\nWe follow the setup of~ on GPT-2 for a direct comparison and add WikiSQL~ (NL to SQL queries) and SAMSum~ (conversation summarization) for large-scale experiments on GPT-3.\nSee  for more details on the datasets we use.\nWe use NVIDIA Tesla V100 for all experiments.\n"}, {"title": "Baselines", "content": "To compare with other baselines broadly, we replicate the setups used by prior work and reuse their reported numbers whenever possible.\nThis, however, means that some baselines might only appear in certain experiments.\nFine-Tuning (FT) is a common approach for adaptation.\nDuring fine-tuning, the model is initialized to the pre-trained weights and biases, and all model parameters undergo gradient updates.%\nA simple variant is to update only some layers while freezing others.\nWe include one such baseline reported in prior work~ on GPT-2, which adapts just the last two layers ($FT^{Top2}$).\nBias-only or BitFit is a baseline where we only train the bias vectors while freezing everything else.\nContemporarily, this baseline has also been studied by BitFit~.\nPrefix-embedding tuning (PreEmbed) inserts special tokens among the input tokens.\nThese special tokens have trainable word embeddings and are generally not in the model's vocabulary.\nWhere to place such tokens can have an impact on performance.\nWe focus on ``prefixing'', which prepends such tokens to the prompt, and ``infixing'', which appends to the prompt; both are discussed in~.\nWe use $l_{p}$ (resp. $l_{i}$) denote the number of prefix (resp. infix) tokens.\nThe number of trainable parameters is $|= d_{model} (l_p + l_i)$. \nPrefix-layer tuning (PreLayer) is an extension to prefix-embedding tuning.\nInstead of just learning the word embeddings (or equivalently, the activations after the embedding layer) for some special tokens, we learn the activations after every Transformer layer.\nThe activations computed from previous layers are simply replaced by trainable ones.\nThe resulting number of trainable parameters is $|= L d_{model} (l_p + l_i)$, where $L$ is the number of Transformer layers.\nAdapter tuning as proposed in~ inserts adapter layers between the self-attention module (and the MLP module) and the subsequent residual connection.\nThere are two fully connected layers with biases in an adapter layer with a nonlinearity in between.\nWe call this original design $Adapter^{H}$.\nRecently,  proposed a more efficient design with the adapter layer applied only after the MLP module and after a LayerNorm.\nWe call it $Adapter^{L}$.\nThis is very similar to another deign proposed in~, which we call $Adapter^{P}$.\nWe also include another baseline call AdapterDrop~ which drops some adapter layers for greater efficiency ($Adapter^{D}$).\nWe cite numbers from prior works whenever possible to maximize the number of baselines we compare with; they are in rows with an asterisk (*) in the first column.\nIn all cases,  we have $|= _{Adpt} (2 d_{model} r + r + d_{model}) + 2 _{LN} d_{model}$ where $_{Adpt}$ is the number of adapter layers and $_{LN}$ the number of trainable LayerNorms (e.g., in $^{}$).\nLoRA adds trainable pairs of rank decomposition matrices in parallel to existing weight matrices.\nAs mentioned in~, we only apply LoRA to $W_{q}$ and $W_{v}$ in most experiments for simplicity.\nThe number of trainable parameters is determined by the rank $r$ and the shape of the original weights: $|= 2 _{LoRA} d_{model} r$, where $_{LoRA}$ is the number of weight matrices we apply LoRA to.\n"}, {"title": "RoBERTa base/large", "content": ""}, {"title": "DeBERTa XXL", "content": ""}, {"title": "GPT-2 medium/large", "content": ""}, {"title": "", "content": ""}, {"title": "Low-Rank-Parametrized Update Matrices", "content": ""}, {"title": "", "content": ""}, {"title": "Which Weight Matrices in Transformer Should We Apply LoRA to?", "content": ""}, {"title": "", "content": ""}, {"title": "RoBERTa", "content": ""}, {"title": "DeBERTa", "content": ""}, {"title": "GPT-2", "content": ""}, {"title": "", "content": ""}, {"title": "Additional Experiments on GPT-2", "content": ""}, {"title": "Additional Experiments on GPT-3", "content": ""}, {"title": "", "content": ""}, {"title": "Correlation between LoRA Modules", "content": ""}, {"title": "Effect of $r$ on GPT-2", "content": ""}, {"title": "Amplification Factor", "content": "{{(Left)}}\n{{(Center)}}\n{{(Right)}}\n{{(Top)}}\n{{(Bottom)}}\n{{(a)}}\n{{(b)}}\n{{(c)}}\n{{(d)}}\n[1]{{#1}}\n}\n}\n and }\n, ,  and }\n}\n}\n and }\n,  and }\n}\n}\n}\n}\n}\n--}\n}\n}\n and }\n and }\n}\n}\n and }\n}\n{}\n{}}}\n{}}}\n}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}\n}\n}\n}\n}\n}\n}\n}\n}\n}\n}\n}\n}\n}\n}\n}\n}\n}\n}\n}\n}\n}\n}\n}\n}\n}\n}\n}\n}\n}\n}\n}\n}\n}\n}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n{{{m}{sl}\n{bold}{{{bx}{n}\n[1]{}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}\n}\n}\n}\n}\n}\n}\n}\n}\n}\n}\n}\n}\n}\n}\n}\n}\n}\n}\n}\n}\n}\n}\n}\n}\n}\n}\n}\n[1]{}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n{p_{}}\n{_{}}\n{_{}}\n{p_{}}\n{P_{}}\n{_{}}\n{p_{}}\n{p_{}}\n{p_{}}\n{} %\n{}\n{}\n{}\n{}\n{\n{\n{}\n{}\n{\n{\n{D_{}}\n{}\n{}\n{}\n{L^0}\n{L^1}\n{L^2}\n{L^p}\n{L^\n{Pa} %\n{arg\n{arg\n{sign}\n{Tr}\n"}]}, {"title": "Empirical Experiments", "subsections": [{"title": "", "content": "\nWe evaluate the downstream task performance of LoRA on RoBERTa~, DeBERTa~, and GPT-2~, before scaling up to GPT-3 175B~.\nOur experiments cover a wide range of tasks, from natural language understanding (NLU) to generation (NLG).\nSpecifically, we evaluate on the GLUE~ benchmark for RoBERTa and DeBERTa.\nWe follow the setup of~ on GPT-2 for a direct comparison and add WikiSQL~ (NL to SQL queries) and SAMSum~ (conversation summarization) for large-scale experiments on GPT-3.\nSee  for more details on the datasets we use.\nWe use NVIDIA Tesla V100 for all experiments.\n"}, {"title": "Baselines", "content": "To compare with other baselines broadly, we replicate the setups used by prior work and reuse their reported numbers whenever possible.\nThis, however, means that some baselines might only appear in certain experiments.\nFine-Tuning (FT) is a common approach for adaptation.\nDuring fine-tuning, the model is initialized to the pre-trained weights and biases, and all model parameters undergo gradient updates.%\nA simple variant is to update only some layers while freezing others.\nWe include one such baseline reported in prior work~ on GPT-2, which adapts just the last two layers ($FT^{Top2}$).\nBias-only or BitFit is a baseline where we only train the bias vectors while freezing everything else.\nContemporarily, this baseline has also been studied by BitFit~.\nPrefix-embedding tuning (PreEmbed) inserts special tokens among the input tokens.\nThese special tokens have trainable word embeddings and are generally not in the model's vocabulary.\nWhere to place such tokens can have an impact on performance.\nWe focus on ``prefixing'', which prepends such tokens to the prompt, and ``infixing'', which appends to the prompt; both are discussed in~.\nWe use $l_{p}$ (resp. $l_{i}$) denote the number of prefix (resp. infix) tokens.\nThe number of trainable parameters is $|= d_{model} (l_p + l_i)$. \nPrefix-layer tuning (PreLayer) is an extension to prefix-embedding tuning.\nInstead of just learning the word embeddings (or equivalently, the activations after the embedding layer) for some special tokens, we learn the activations after every Transformer layer.\nThe activations computed from previous layers are simply replaced by trainable ones.\nThe resulting number of trainable parameters is $|= L d_{model} (l_p + l_i)$, where $L$ is the number of Transformer layers.\nAdapter tuning as proposed in~ inserts adapter layers between the self-attention module (and the MLP module) and the subsequent residual connection.\nThere are two fully connected layers with biases in an adapter layer with a nonlinearity in between.\nWe call this original design $Adapter^{H}$.\nRecently,  proposed a more efficient design with the adapter layer applied only after the MLP module and after a LayerNorm.\nWe call it $Adapter^{L}$.\nThis is very similar to another deign proposed in~, which we call $Adapter^{P}$.\nWe also include another baseline call AdapterDrop~ which drops some adapter layers for greater efficiency ($Adapter^{D}$).\nWe cite numbers from prior works whenever possible to maximize the number of baselines we compare with; they are in rows with an asterisk (*) in the first column.\nIn all cases,  we have $|= _{Adpt} (2 d_{model} r + r + d_{model}) + 2 _{LN} d_{model}$ where $_{Adpt}$ is the number of adapter layers and $_{LN}$ the number of trainable LayerNorms (e.g., in $^{}$).\nLoRA adds trainable pairs of rank decomposition matrices in parallel to existing weight matrices.\nAs mentioned in~, we only apply LoRA to $W_{q}$ and $W_{v}$ in most experiments for simplicity.\nThe number of trainable parameters is determined by the rank $r$ and the shape of the original weights: $|= 2 _{LoRA} d_{model} r$, where $_{LoRA}$ is the number of weight matrices we apply LoRA to.\n"}, {"title": "RoBERTa base/large", "content": ""}, {"title": "DeBERTa XXL", "content": ""}, {"title": "GPT-2 medium/large", "content": ""}, {"title": "", "content": ""}, {"title": "Low-Rank-Parametrized Update Matrices", "content": ""}, {"title": "", "content": ""}, {"title": "Which Weight Matrices in Transformer Should We Apply LoRA to?", "content": ""}, {"title": "", "content": ""}, {"title": "RoBERTa", "content": ""}, {"title": "DeBERTa", "content": ""}, {"title": "GPT-2", "content": ""}, {"title": "", "content": ""}, {"title": "Additional Experiments on GPT-2", "content": ""}, {"title": "Additional Experiments on GPT-3", "content": ""}, {"title": "", "content": ""}, {"title": "Correlation between LoRA Modules", "content": ""}, {"title": "Effect of $r$ on GPT-2", "content": ""}, {"title": "Amplification Factor", "content": "{{(Left)}}\n{{(Center)}}\n{{(Right)}}\n{{(Top)}}\n{{(Bottom)}}\n{{(a)}}\n{{(b)}}\n{{(c)}}\n{{(d)}}\n[1]{{#1}}\n}\n}\n and }\n, ,  and }\n}\n}\n and }\n,  and }\n}\n}\n}\n}\n}\n--}\n}\n}\n and }\n and }\n}\n}\n and }\n}\n{}\n{}}}\n{}}}\n}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}\n}\n}\n}\n}\n}\n}\n}\n}\n}\n}\n}\n}\n}\n}\n}\n}\n}\n}\n}\n}\n}\n}\n}\n}\n}\n}\n}\n}\n}\n}\n}\n}\n}\n}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n{{{m}{sl}\n{bold}{{{bx}{n}\n[1]{}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}\n}\n}\n}\n}\n}\n}\n}\n}\n}\n}\n}\n}\n}\n}\n}\n}\n}\n}\n}\n}\n}\n}\n}\n}\n}\n}\n}\n[1]{}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n{p_{}}\n{_{}}\n{_{}}\n{p_{}}\n{P_{}}\n{_{}}\n{p_{}}\n{p_{}}\n{p_{}}\n{} %\n{}\n{}\n{}\n{}\n{\n{\n{}\n{}\n{\n{\n{D_{}}\n{}\n{}\n{}\n{L^0}\n{L^1}\n{L^2}\n{L^p}\n{L^\n{Pa} %\n{arg\n{arg\n{sign}\n{Tr}\n"}]}, {"title": "Introduction", "subsections": [{"title": "", "content": "\nWe evaluate the downstream task performance of LoRA on RoBERTa~, DeBERTa~, and GPT-2~, before scaling up to GPT-3 175B~.\nOur experiments cover a wide range of tasks, from natural language understanding (NLU) to generation (NLG).\nSpecifically, we evaluate on the GLUE~ benchmark for RoBERTa and DeBERTa.\nWe follow the setup of~ on GPT-2 for a direct comparison and add WikiSQL~ (NL to SQL queries) and SAMSum~ (conversation summarization) for large-scale experiments on GPT-3.\nSee  for more details on the datasets we use.\nWe use NVIDIA Tesla V100 for all experiments.\n"}, {"title": "Baselines", "content": "To compare with other baselines broadly, we replicate the setups used by prior work and reuse their reported numbers whenever possible.\nThis, however, means that some baselines might only appear in certain experiments.\nFine-Tuning (FT) is a common approach for adaptation.\nDuring fine-tuning, the model is initialized to the pre-trained weights and biases, and all model parameters undergo gradient updates.%\nA simple variant is to update only some layers while freezing others.\nWe include one such baseline reported in prior work~ on GPT-2, which adapts just the last two layers ($FT^{Top2}$).\nBias-only or BitFit is a baseline where we only train the bias vectors while freezing everything else.\nContemporarily, this baseline has also been studied by BitFit~.\nPrefix-embedding tuning (PreEmbed) inserts special tokens among the input tokens.\nThese special tokens have trainable word embeddings and are generally not in the model's vocabulary.\nWhere to place such tokens can have an impact on performance.\nWe focus on ``prefixing'', which prepends such tokens to the prompt, and ``infixing'', which appends to the prompt; both are discussed in~.\nWe use $l_{p}$ (resp. $l_{i}$) denote the number of prefix (resp. infix) tokens.\nThe number of trainable parameters is $|= d_{model} (l_p + l_i)$. \nPrefix-layer tuning (PreLayer) is an extension to prefix-embedding tuning.\nInstead of just learning the word embeddings (or equivalently, the activations after the embedding layer) for some special tokens, we learn the activations after every Transformer layer.\nThe activations computed from previous layers are simply replaced by trainable ones.\nThe resulting number of trainable parameters is $|= L d_{model} (l_p + l_i)$, where $L$ is the number of Transformer layers.\nAdapter tuning as proposed in~ inserts adapter layers between the self-attention module (and the MLP module) and the subsequent residual connection.\nThere are two fully connected layers with biases in an adapter layer with a nonlinearity in between.\nWe call this original design $Adapter^{H}$.\nRecently,  proposed a more efficient design with the adapter layer applied only after the MLP module and after a LayerNorm.\nWe call it $Adapter^{L}$.\nThis is very similar to another deign proposed in~, which we call $Adapter^{P}$.\nWe also include another baseline call AdapterDrop~ which drops some adapter layers for greater efficiency ($Adapter^{D}$).\nWe cite numbers from prior works whenever possible to maximize the number of baselines we compare with; they are in rows with an asterisk (*) in the first column.\nIn all cases,  we have $|= _{Adpt} (2 d_{model} r + r + d_{model}) + 2 _{LN} d_{model}$ where $_{Adpt}$ is the number of adapter layers and $_{LN}$ the number of trainable LayerNorms (e.g., in $^{}$).\nLoRA adds trainable pairs of rank decomposition matrices in parallel to existing weight matrices.\nAs mentioned in~, we only apply LoRA to $W_{q}$ and $W_{v}$ in most experiments for simplicity.\nThe number of trainable parameters is determined by the rank $r$ and the shape of the original weights: $|= 2 _{LoRA} d_{model} r$, where $_{LoRA}$ is the number of weight matrices we apply LoRA to.\n"}, {"title": "RoBERTa base/large", "content": ""}, {"title": "DeBERTa XXL", "content": ""}, {"title": "GPT-2 medium/large", "content": ""}, {"title": "", "content": ""}, {"title": "Low-Rank-Parametrized Update Matrices", "content": ""}, {"title": "", "content": ""}, {"title": "Which Weight Matrices in Transformer Should We Apply LoRA to?", "content": ""}, {"title": "", "content": ""}, {"title": "RoBERTa", "content": ""}, {"title": "DeBERTa", "content": ""}, {"title": "GPT-2", "content": ""}, {"title": "", "content": ""}, {"title": "Additional Experiments on GPT-2", "content": ""}, {"title": "Additional Experiments on GPT-3", "content": ""}, {"title": "", "content": ""}, {"title": "Correlation between LoRA Modules", "content": ""}, {"title": "Effect of $r$ on GPT-2", "content": ""}, {"title": "Amplification Factor", "content": "{{(Left)}}\n{{(Center)}}\n{{(Right)}}\n{{(Top)}}\n{{(Bottom)}}\n{{(a)}}\n{{(b)}}\n{{(c)}}\n{{(d)}}\n[1]{{#1}}\n}\n}\n and }\n, ,  and }\n}\n}\n and }\n,  and }\n}\n}\n}\n}\n}\n--}\n}\n}\n and }\n and }\n}\n}\n and }\n}\n{}\n{}}}\n{}}}\n}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}\n}\n}\n}\n}\n}\n}\n}\n}\n}\n}\n}\n}\n}\n}\n}\n}\n}\n}\n}\n}\n}\n}\n}\n}\n}\n}\n}\n}\n}\n}\n}\n}\n}\n}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n{{{m}{sl}\n{bold}{{{bx}{n}\n[1]{}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}\n}\n}\n}\n}\n}\n}\n}\n}\n}\n}\n}\n}\n}\n}\n}\n}\n}\n}\n}\n}\n}\n}\n}\n}\n}\n}\n}\n[1]{}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n{p_{}}\n{_{}}\n{_{}}\n{p_{}}\n{P_{}}\n{_{}}\n{p_{}}\n{p_{}}\n{p_{}}\n{} %\n{}\n{}\n{}\n{}\n{\n{\n{}\n{}\n{\n{\n{D_{}}\n{}\n{}\n{}\n{L^0}\n{L^1}\n{L^2}\n{L^p}\n{L^\n{Pa} %\n{arg\n{arg\n{sign}\n{Tr}\n"}]}, {"title": "Problem Statement", "subsections": [{"title": "", "content": "\nWe evaluate the downstream task performance of LoRA on RoBERTa~, DeBERTa~, and GPT-2~, before scaling up to GPT-3 175B~.\nOur experiments cover a wide range of tasks, from natural language understanding (NLU) to generation (NLG).\nSpecifically, we evaluate on the GLUE~ benchmark for RoBERTa and DeBERTa.\nWe follow the setup of~ on GPT-2 for a direct comparison and add WikiSQL~ (NL to SQL queries) and SAMSum~ (conversation summarization) for large-scale experiments on GPT-3.\nSee  for more details on the datasets we use.\nWe use NVIDIA Tesla V100 for all experiments.\n"}, {"title": "Baselines", "content": "To compare with other baselines broadly, we replicate the setups used by prior work and reuse their reported numbers whenever possible.\nThis, however, means that some baselines might only appear in certain experiments.\nFine-Tuning (FT) is a common approach for adaptation.\nDuring fine-tuning, the model is initialized to the pre-trained weights and biases, and all model parameters undergo gradient updates.%\nA simple variant is to update only some layers while freezing others.\nWe include one such baseline reported in prior work~ on GPT-2, which adapts just the last two layers ($FT^{Top2}$).\nBias-only or BitFit is a baseline where we only train the bias vectors while freezing everything else.\nContemporarily, this baseline has also been studied by BitFit~.\nPrefix-embedding tuning (PreEmbed) inserts special tokens among the input tokens.\nThese special tokens have trainable word embeddings and are generally not in the model's vocabulary.\nWhere to place such tokens can have an impact on performance.\nWe focus on ``prefixing'', which prepends such tokens to the prompt, and ``infixing'', which appends to the prompt; both are discussed in~.\nWe use $l_{p}$ (resp. $l_{i}$) denote the number of prefix (resp. infix) tokens.\nThe number of trainable parameters is $|= d_{model} (l_p + l_i)$. \nPrefix-layer tuning (PreLayer) is an extension to prefix-embedding tuning.\nInstead of just learning the word embeddings (or equivalently, the activations after the embedding layer) for some special tokens, we learn the activations after every Transformer layer.\nThe activations computed from previous layers are simply replaced by trainable ones.\nThe resulting number of trainable parameters is $|= L d_{model} (l_p + l_i)$, where $L$ is the number of Transformer layers.\nAdapter tuning as proposed in~ inserts adapter layers between the self-attention module (and the MLP module) and the subsequent residual connection.\nThere are two fully connected layers with biases in an adapter layer with a nonlinearity in between.\nWe call this original design $Adapter^{H}$.\nRecently,  proposed a more efficient design with the adapter layer applied only after the MLP module and after a LayerNorm.\nWe call it $Adapter^{L}$.\nThis is very similar to another deign proposed in~, which we call $Adapter^{P}$.\nWe also include another baseline call AdapterDrop~ which drops some adapter layers for greater efficiency ($Adapter^{D}$).\nWe cite numbers from prior works whenever possible to maximize the number of baselines we compare with; they are in rows with an asterisk (*) in the first column.\nIn all cases,  we have $|= _{Adpt} (2 d_{model} r + r + d_{model}) + 2 _{LN} d_{model}$ where $_{Adpt}$ is the number of adapter layers and $_{LN}$ the number of trainable LayerNorms (e.g., in $^{}$).\nLoRA adds trainable pairs of rank decomposition matrices in parallel to existing weight matrices.\nAs mentioned in~, we only apply LoRA to $W_{q}$ and $W_{v}$ in most experiments for simplicity.\nThe number of trainable parameters is determined by the rank $r$ and the shape of the original weights: $|= 2 _{LoRA} d_{model} r$, where $_{LoRA}$ is the number of weight matrices we apply LoRA to.\n"}, {"title": "RoBERTa base/large", "content": ""}, {"title": "DeBERTa XXL", "content": ""}, {"title": "GPT-2 medium/large", "content": ""}, {"title": "", "content": ""}, {"title": "Low-Rank-Parametrized Update Matrices", "content": ""}, {"title": "", "content": ""}, {"title": "Which Weight Matrices in Transformer Should We Apply LoRA to?", "content": ""}, {"title": "", "content": ""}, {"title": "RoBERTa", "content": ""}, {"title": "DeBERTa", "content": ""}, {"title": "GPT-2", "content": ""}, {"title": "", "content": ""}, {"title": "Additional Experiments on GPT-2", "content": ""}, {"title": "Additional Experiments on GPT-3", "content": ""}, {"title": "", "content": ""}, {"title": "Correlation between LoRA Modules", "content": ""}, {"title": "Effect of $r$ on GPT-2", "content": ""}, {"title": "Amplification Factor", "content": "{{(Left)}}\n{{(Center)}}\n{{(Right)}}\n{{(Top)}}\n{{(Bottom)}}\n{{(a)}}\n{{(b)}}\n{{(c)}}\n{{(d)}}\n[1]{{#1}}\n}\n}\n and }\n, ,  and }\n}\n}\n and }\n,  and }\n}\n}\n}\n}\n}\n--}\n}\n}\n and }\n and }\n}\n}\n and }\n}\n{}\n{}}}\n{}}}\n}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}\n}\n}\n}\n}\n}\n}\n}\n}\n}\n}\n}\n}\n}\n}\n}\n}\n}\n}\n}\n}\n}\n}\n}\n}\n}\n}\n}\n}\n}\n}\n}\n}\n}\n}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n{{{m}{sl}\n{bold}{{{bx}{n}\n[1]{}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}\n}\n}\n}\n}\n}\n}\n}\n}\n}\n}\n}\n}\n}\n}\n}\n}\n}\n}\n}\n}\n}\n}\n}\n}\n}\n}\n}\n[1]{}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n{p_{}}\n{_{}}\n{_{}}\n{p_{}}\n{P_{}}\n{_{}}\n{p_{}}\n{p_{}}\n{p_{}}\n{} %\n{}\n{}\n{}\n{}\n{\n{\n{}\n{}\n{\n{\n{D_{}}\n{}\n{}\n{}\n{L^0}\n{L^1}\n{L^2}\n{L^p}\n{L^\n{Pa} %\n{arg\n{arg\n{sign}\n{Tr}\n"}]}, {"title": "Aren't Existing Solutions Good Enough?", "subsections": [{"title": "", "content": "\nWe evaluate the downstream task performance of LoRA on RoBERTa~, DeBERTa~, and GPT-2~, before scaling up to GPT-3 175B~.\nOur experiments cover a wide range of tasks, from natural language understanding (NLU) to generation (NLG).\nSpecifically, we evaluate on the GLUE~ benchmark for RoBERTa and DeBERTa.\nWe follow the setup of~ on GPT-2 for a direct comparison and add WikiSQL~ (NL to SQL queries) and SAMSum~ (conversation summarization) for large-scale experiments on GPT-3.\nSee  for more details on the datasets we use.\nWe use NVIDIA Tesla V100 for all experiments.\n"}, {"title": "Baselines", "content": "To compare with other baselines broadly, we replicate the setups used by prior work and reuse their reported numbers whenever possible.\nThis, however, means that some baselines might only appear in certain experiments.\nFine-Tuning (FT) is a common approach for adaptation.\nDuring fine-tuning, the model is initialized to the pre-trained weights and biases, and all model parameters undergo gradient updates.%\nA simple variant is to update only some layers while freezing others.\nWe include one such baseline reported in prior work~ on GPT-2, which adapts just the last two layers ($FT^{Top2}$).\nBias-only or BitFit is a baseline where we only train the bias vectors while freezing everything else.\nContemporarily, this baseline has also been studied by BitFit~.\nPrefix-embedding tuning (PreEmbed) inserts special tokens among the input tokens.\nThese special tokens have trainable word embeddings and are generally not in the model's vocabulary.\nWhere to place such tokens can have an impact on performance.\nWe focus on ``prefixing'', which prepends such tokens to the prompt, and ``infixing'', which appends to the prompt; both are discussed in~.\nWe use $l_{p}$ (resp. $l_{i}$) denote the number of prefix (resp. infix) tokens.\nThe number of trainable parameters is $|= d_{model} (l_p + l_i)$. \nPrefix-layer tuning (PreLayer) is an extension to prefix-embedding tuning.\nInstead of just learning the word embeddings (or equivalently, the activations after the embedding layer) for some special tokens, we learn the activations after every Transformer layer.\nThe activations computed from previous layers are simply replaced by trainable ones.\nThe resulting number of trainable parameters is $|= L d_{model} (l_p + l_i)$, where $L$ is the number of Transformer layers.\nAdapter tuning as proposed in~ inserts adapter layers between the self-attention module (and the MLP module) and the subsequent residual connection.\nThere are two fully connected layers with biases in an adapter layer with a nonlinearity in between.\nWe call this original design $Adapter^{H}$.\nRecently,  proposed a more efficient design with the adapter layer applied only after the MLP module and after a LayerNorm.\nWe call it $Adapter^{L}$.\nThis is very similar to another deign proposed in~, which we call $Adapter^{P}$.\nWe also include another baseline call AdapterDrop~ which drops some adapter layers for greater efficiency ($Adapter^{D}$).\nWe cite numbers from prior works whenever possible to maximize the number of baselines we compare with; they are in rows with an asterisk (*) in the first column.\nIn all cases,  we have $|= _{Adpt} (2 d_{model} r + r + d_{model}) + 2 _{LN} d_{model}$ where $_{Adpt}$ is the number of adapter layers and $_{LN}$ the number of trainable LayerNorms (e.g., in $^{}$).\nLoRA adds trainable pairs of rank decomposition matrices in parallel to existing weight matrices.\nAs mentioned in~, we only apply LoRA to $W_{q}$ and $W_{v}$ in most experiments for simplicity.\nThe number of trainable parameters is determined by the rank $r$ and the shape of the original weights: $|= 2 _{LoRA} d_{model} r$, where $_{LoRA}$ is the number of weight matrices we apply LoRA to.\n"}, {"title": "RoBERTa base/large", "content": ""}, {"title": "DeBERTa XXL", "content": ""}, {"title": "GPT-2 medium/large", "content": ""}, {"title": "", "content": ""}, {"title": "Low-Rank-Parametrized Update Matrices", "content": ""}, {"title": "", "content": ""}, {"title": "Which Weight Matrices in Transformer Should We Apply LoRA to?", "content": ""}, {"title": "", "content": ""}, {"title": "RoBERTa", "content": ""}, {"title": "DeBERTa", "content": ""}, {"title": "GPT-2", "content": ""}, {"title": "", "content": ""}, {"title": "Additional Experiments on GPT-2", "content": ""}, {"title": "Additional Experiments on GPT-3", "content": ""}, {"title": "", "content": ""}, {"title": "Correlation between LoRA Modules", "content": ""}, {"title": "Effect of $r$ on GPT-2", "content": ""}, {"title": "Amplification Factor", "content": "{{(Left)}}\n{{(Center)}}\n{{(Right)}}\n{{(Top)}}\n{{(Bottom)}}\n{{(a)}}\n{{(b)}}\n{{(c)}}\n{{(d)}}\n[1]{{#1}}\n}\n}\n and }\n, ,  and }\n}\n}\n and }\n,  and }\n}\n}\n}\n}\n}\n--}\n}\n}\n and }\n and }\n}\n}\n and }\n}\n{}\n{}}}\n{}}}\n}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}\n}\n}\n}\n}\n}\n}\n}\n}\n}\n}\n}\n}\n}\n}\n}\n}\n}\n}\n}\n}\n}\n}\n}\n}\n}\n}\n}\n}\n}\n}\n}\n}\n}\n}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n{{{m}{sl}\n{bold}{{{bx}{n}\n[1]{}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}\n}\n}\n}\n}\n}\n}\n}\n}\n}\n}\n}\n}\n}\n}\n}\n}\n}\n}\n}\n}\n}\n}\n}\n}\n}\n}\n}\n[1]{}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n{p_{}}\n{_{}}\n{_{}}\n{p_{}}\n{P_{}}\n{_{}}\n{p_{}}\n{p_{}}\n{p_{}}\n{} %\n{}\n{}\n{}\n{}\n{\n{\n{}\n{}\n{\n{\n{D_{}}\n{}\n{}\n{}\n{L^0}\n{L^1}\n{L^2}\n{L^p}\n{L^\n{Pa} %\n{arg\n{arg\n{sign}\n{Tr}\n"}]}, {"title": "Our Method", "subsections": [{"title": "", "content": "\nWe evaluate the downstream task performance of LoRA on RoBERTa~, DeBERTa~, and GPT-2~, before scaling up to GPT-3 175B~.\nOur experiments cover a wide range of tasks, from natural language understanding (NLU) to generation (NLG).\nSpecifically, we evaluate on the GLUE~ benchmark for RoBERTa and DeBERTa.\nWe follow the setup of~ on GPT-2 for a direct comparison and add WikiSQL~ (NL to SQL queries) and SAMSum~ (conversation summarization) for large-scale experiments on GPT-3.\nSee  for more details on the datasets we use.\nWe use NVIDIA Tesla V100 for all experiments.\n"}, {"title": "Baselines", "content": "To compare with other baselines broadly, we replicate the setups used by prior work and reuse their reported numbers whenever possible.\nThis, however, means that some baselines might only appear in certain experiments.\nFine-Tuning (FT) is a common approach for adaptation.\nDuring fine-tuning, the model is initialized to the pre-trained weights and biases, and all model parameters undergo gradient updates.%\nA simple variant is to update only some layers while freezing others.\nWe include one such baseline reported in prior work~ on GPT-2, which adapts just the last two layers ($FT^{Top2}$).\nBias-only or BitFit is a baseline where we only train the bias vectors while freezing everything else.\nContemporarily, this baseline has also been studied by BitFit~.\nPrefix-embedding tuning (PreEmbed) inserts special tokens among the input tokens.\nThese special tokens have trainable word embeddings and are generally not in the model's vocabulary.\nWhere to place such tokens can have an impact on performance.\nWe focus on ``prefixing'', which prepends such tokens to the prompt, and ``infixing'', which appends to the prompt; both are discussed in~.\nWe use $l_{p}$ (resp. $l_{i}$) denote the number of prefix (resp. infix) tokens.\nThe number of trainable parameters is $|= d_{model} (l_p + l_i)$. \nPrefix-layer tuning (PreLayer) is an extension to prefix-embedding tuning.\nInstead of just learning the word embeddings (or equivalently, the activations after the embedding layer) for some special tokens, we learn the activations after every Transformer layer.\nThe activations computed from previous layers are simply replaced by trainable ones.\nThe resulting number of trainable parameters is $|= L d_{model} (l_p + l_i)$, where $L$ is the number of Transformer layers.\nAdapter tuning as proposed in~ inserts adapter layers between the self-attention module (and the MLP module) and the subsequent residual connection.\nThere are two fully connected layers with biases in an adapter layer with a nonlinearity in between.\nWe call this original design $Adapter^{H}$.\nRecently,  proposed a more efficient design with the adapter layer applied only after the MLP module and after a LayerNorm.\nWe call it $Adapter^{L}$.\nThis is very similar to another deign proposed in~, which we call $Adapter^{P}$.\nWe also include another baseline call AdapterDrop~ which drops some adapter layers for greater efficiency ($Adapter^{D}$).\nWe cite numbers from prior works whenever possible to maximize the number of baselines we compare with; they are in rows with an asterisk (*) in the first column.\nIn all cases,  we have $|= _{Adpt} (2 d_{model} r + r + d_{model}) + 2 _{LN} d_{model}$ where $_{Adpt}$ is the number of adapter layers and $_{LN}$ the number of trainable LayerNorms (e.g., in $^{}$).\nLoRA adds trainable pairs of rank decomposition matrices in parallel to existing weight matrices.\nAs mentioned in~, we only apply LoRA to $W_{q}$ and $W_{v}$ in most experiments for simplicity.\nThe number of trainable parameters is determined by the rank $r$ and the shape of the original weights: $|= 2 _{LoRA} d_{model} r$, where $_{LoRA}$ is the number of weight matrices we apply LoRA to.\n"}, {"title": "RoBERTa base/large", "content": ""}, {"title": "DeBERTa XXL", "content": ""}, {"title": "GPT-2 medium/large", "content": ""}, {"title": "", "content": ""}, {"title": "Low-Rank-Parametrized Update Matrices", "content": ""}, {"title": "", "content": ""}, {"title": "Which Weight Matrices in Transformer Should We Apply LoRA to?", "content": ""}, {"title": "", "content": ""}, {"title": "RoBERTa", "content": ""}, {"title": "DeBERTa", "content": ""}, {"title": "GPT-2", "content": ""}, {"title": "", "content": ""}, {"title": "Additional Experiments on GPT-2", "content": ""}, {"title": "Additional Experiments on GPT-3", "content": ""}, {"title": "", "content": ""}, {"title": "Correlation between LoRA Modules", "content": ""}, {"title": "Effect of $r$ on GPT-2", "content": ""}, {"title": "Amplification Factor", "content": "{{(Left)}}\n{{(Center)}}\n{{(Right)}}\n{{(Top)}}\n{{(Bottom)}}\n{{(a)}}\n{{(b)}}\n{{(c)}}\n{{(d)}}\n[1]{{#1}}\n}\n}\n and }\n, ,  and }\n}\n}\n and }\n,  and }\n}\n}\n}\n}\n}\n--}\n}\n}\n and }\n and }\n}\n}\n and }\n}\n{}\n{}}}\n{}}}\n}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}\n}\n}\n}\n}\n}\n}\n}\n}\n}\n}\n}\n}\n}\n}\n}\n}\n}\n}\n}\n}\n}\n}\n}\n}\n}\n}\n}\n}\n}\n}\n}\n}\n}\n}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n{{{m}{sl}\n{bold}{{{bx}{n}\n[1]{}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}\n}\n}\n}\n}\n}\n}\n}\n}\n}\n}\n}\n}\n}\n}\n}\n}\n}\n}\n}\n}\n}\n}\n}\n}\n}\n}\n}\n[1]{}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n{p_{}}\n{_{}}\n{_{}}\n{p_{}}\n{P_{}}\n{_{}}\n{p_{}}\n{p_{}}\n{p_{}}\n{} %\n{}\n{}\n{}\n{}\n{\n{\n{}\n{}\n{\n{\n{D_{}}\n{}\n{}\n{}\n{L^0}\n{L^1}\n{L^2}\n{L^p}\n{L^\n{Pa} %\n{arg\n{arg\n{sign}\n{Tr}\n"}]}, {"title": "Related Works", "subsections": [{"title": "", "content": "\nWe evaluate the downstream task performance of LoRA on RoBERTa~, DeBERTa~, and GPT-2~, before scaling up to GPT-3 175B~.\nOur experiments cover a wide range of tasks, from natural language understanding (NLU) to generation (NLG).\nSpecifically, we evaluate on the GLUE~ benchmark for RoBERTa and DeBERTa.\nWe follow the setup of~ on GPT-2 for a direct comparison and add WikiSQL~ (NL to SQL queries) and SAMSum~ (conversation summarization) for large-scale experiments on GPT-3.\nSee  for more details on the datasets we use.\nWe use NVIDIA Tesla V100 for all experiments.\n"}, {"title": "Baselines", "content": "To compare with other baselines broadly, we replicate the setups used by prior work and reuse their reported numbers whenever possible.\nThis, however, means that some baselines might only appear in certain experiments.\nFine-Tuning (FT) is a common approach for adaptation.\nDuring fine-tuning, the model is initialized to the pre-trained weights and biases, and all model parameters undergo gradient updates.%\nA simple variant is to update only some layers while freezing others.\nWe include one such baseline reported in prior work~ on GPT-2, which adapts just the last two layers ($FT^{Top2}$).\nBias-only or BitFit is a baseline where we only train the bias vectors while freezing everything else.\nContemporarily, this baseline has also been studied by BitFit~.\nPrefix-embedding tuning (PreEmbed) inserts special tokens among the input tokens.\nThese special tokens have trainable word embeddings and are generally not in the model's vocabulary.\nWhere to place such tokens can have an impact on performance.\nWe focus on ``prefixing'', which prepends such tokens to the prompt, and ``infixing'', which appends to the prompt; both are discussed in~.\nWe use $l_{p}$ (resp. $l_{i}$) denote the number of prefix (resp. infix) tokens.\nThe number of trainable parameters is $|= d_{model} (l_p + l_i)$. \nPrefix-layer tuning (PreLayer) is an extension to prefix-embedding tuning.\nInstead of just learning the word embeddings (or equivalently, the activations after the embedding layer) for some special tokens, we learn the activations after every Transformer layer.\nThe activations computed from previous layers are simply replaced by trainable ones.\nThe resulting number of trainable parameters is $|= L d_{model} (l_p + l_i)$, where $L$ is the number of Transformer layers.\nAdapter tuning as proposed in~ inserts adapter layers between the self-attention module (and the MLP module) and the subsequent residual connection.\nThere are two fully connected layers with biases in an adapter layer with a nonlinearity in between.\nWe call this original design $Adapter^{H}$.\nRecently,  proposed a more efficient design with the adapter layer applied only after the MLP module and after a LayerNorm.\nWe call it $Adapter^{L}$.\nThis is very similar to another deign proposed in~, which we call $Adapter^{P}$.\nWe also include another baseline call AdapterDrop~ which drops some adapter layers for greater efficiency ($Adapter^{D}$).\nWe cite numbers from prior works whenever possible to maximize the number of baselines we compare with; they are in rows with an asterisk (*) in the first column.\nIn all cases,  we have $|= _{Adpt} (2 d_{model} r + r + d_{model}) + 2 _{LN} d_{model}$ where $_{Adpt}$ is the number of adapter layers and $_{LN}$ the number of trainable LayerNorms (e.g., in $^{}$).\nLoRA adds trainable pairs of rank decomposition matrices in parallel to existing weight matrices.\nAs mentioned in~, we only apply LoRA to $W_{q}$ and $W_{v}$ in most experiments for simplicity.\nThe number of trainable parameters is determined by the rank $r$ and the shape of the original weights: $|= 2 _{LoRA} d_{model} r$, where $_{LoRA}$ is the number of weight matrices we apply LoRA to.\n"}, {"title": "RoBERTa base/large", "content": ""}, {"title": "DeBERTa XXL", "content": ""}, {"title": "GPT-2 medium/large", "content": ""}, {"title": "", "content": ""}, {"title": "Low-Rank-Parametrized Update Matrices", "content": ""}, {"title": "", "content": ""}, {"title": "Which Weight Matrices in Transformer Should We Apply LoRA to?", "content": ""}, {"title": "", "content": ""}, {"title": "RoBERTa", "content": ""}, {"title": "DeBERTa", "content": ""}, {"title": "GPT-2", "content": ""}, {"title": "", "content": ""}, {"title": "Additional Experiments on GPT-2", "content": ""}, {"title": "Additional Experiments on GPT-3", "content": ""}, {"title": "", "content": ""}, {"title": "Correlation between LoRA Modules", "content": ""}, {"title": "Effect of $r$ on GPT-2", "content": ""}, {"title": "Amplification Factor", "content": "{{(Left)}}\n{{(Center)}}\n{{(Right)}}\n{{(Top)}}\n{{(Bottom)}}\n{{(a)}}\n{{(b)}}\n{{(c)}}\n{{(d)}}\n[1]{{#1}}\n}\n}\n and }\n, ,  and }\n}\n}\n and }\n,  and }\n}\n}\n}\n}\n}\n--}\n}\n}\n and }\n and }\n}\n}\n and }\n}\n{}\n{}}}\n{}}}\n}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}\n}\n}\n}\n}\n}\n}\n}\n}\n}\n}\n}\n}\n}\n}\n}\n}\n}\n}\n}\n}\n}\n}\n}\n}\n}\n}\n}\n}\n}\n}\n}\n}\n}\n}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n{{{m}{sl}\n{bold}{{{bx}{n}\n[1]{}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}\n}\n}\n}\n}\n}\n}\n}\n}\n}\n}\n}\n}\n}\n}\n}\n}\n}\n}\n}\n}\n}\n}\n}\n}\n}\n}\n}\n[1]{}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n{p_{}}\n{_{}}\n{_{}}\n{p_{}}\n{P_{}}\n{_{}}\n{p_{}}\n{p_{}}\n{p_{}}\n{} %\n{}\n{}\n{}\n{}\n{\n{\n{}\n{}\n{\n{\n{D_{}}\n{}\n{}\n{}\n{L^0}\n{L^1}\n{L^2}\n{L^p}\n{L^\n{Pa} %\n{arg\n{arg\n{sign}\n{Tr}\n"}]}, {"title": "Understanding the Low-Rank Updates", "subsections": [{"title": "", "content": "\nWe evaluate the downstream task performance of LoRA on RoBERTa~, DeBERTa~, and GPT-2~, before scaling up to GPT-3 175B~.\nOur experiments cover a wide range of tasks, from natural language understanding (NLU) to generation (NLG).\nSpecifically, we evaluate on the GLUE~ benchmark for RoBERTa and DeBERTa.\nWe follow the setup of~ on GPT-2 for a direct comparison and add WikiSQL~ (NL to SQL queries) and SAMSum~ (conversation summarization) for large-scale experiments on GPT-3.\nSee  for more details on the datasets we use.\nWe use NVIDIA Tesla V100 for all experiments.\n"}, {"title": "Baselines", "content": "To compare with other baselines broadly, we replicate the setups used by prior work and reuse their reported numbers whenever possible.\nThis, however, means that some baselines might only appear in certain experiments.\nFine-Tuning (FT) is a common approach for adaptation.\nDuring fine-tuning, the model is initialized to the pre-trained weights and biases, and all model parameters undergo gradient updates.%\nA simple variant is to update only some layers while freezing others.\nWe include one such baseline reported in prior work~ on GPT-2, which adapts just the last two layers ($FT^{Top2}$).\nBias-only or BitFit is a baseline where we only train the bias vectors while freezing everything else.\nContemporarily, this baseline has also been studied by BitFit~.\nPrefix-embedding tuning (PreEmbed) inserts special tokens among the input tokens.\nThese special tokens have trainable word embeddings and are generally not in the model's vocabulary.\nWhere to place such tokens can have an impact on performance.\nWe focus on ``prefixing'', which prepends such tokens to the prompt, and ``infixing'', which appends to the prompt; both are discussed in~.\nWe use $l_{p}$ (resp. $l_{i}$) denote the number of prefix (resp. infix) tokens.\nThe number of trainable parameters is $|= d_{model} (l_p + l_i)$. \nPrefix-layer tuning (PreLayer) is an extension to prefix-embedding tuning.\nInstead of just learning the word embeddings (or equivalently, the activations after the embedding layer) for some special tokens, we learn the activations after every Transformer layer.\nThe activations computed from previous layers are simply replaced by trainable ones.\nThe resulting number of trainable parameters is $|= L d_{model} (l_p + l_i)$, where $L$ is the number of Transformer layers.\nAdapter tuning as proposed in~ inserts adapter layers between the self-attention module (and the MLP module) and the subsequent residual connection.\nThere are two fully connected layers with biases in an adapter layer with a nonlinearity in between.\nWe call this original design $Adapter^{H}$.\nRecently,  proposed a more efficient design with the adapter layer applied only after the MLP module and after a LayerNorm.\nWe call it $Adapter^{L}$.\nThis is very similar to another deign proposed in~, which we call $Adapter^{P}$.\nWe also include another baseline call AdapterDrop~ which drops some adapter layers for greater efficiency ($Adapter^{D}$).\nWe cite numbers from prior works whenever possible to maximize the number of baselines we compare with; they are in rows with an asterisk (*) in the first column.\nIn all cases,  we have $|= _{Adpt} (2 d_{model} r + r + d_{model}) + 2 _{LN} d_{model}$ where $_{Adpt}$ is the number of adapter layers and $_{LN}$ the number of trainable LayerNorms (e.g., in $^{}$).\nLoRA adds trainable pairs of rank decomposition matrices in parallel to existing weight matrices.\nAs mentioned in~, we only apply LoRA to $W_{q}$ and $W_{v}$ in most experiments for simplicity.\nThe number of trainable parameters is determined by the rank $r$ and the shape of the original weights: $|= 2 _{LoRA} d_{model} r$, where $_{LoRA}$ is the number of weight matrices we apply LoRA to.\n"}, {"title": "RoBERTa base/large", "content": ""}, {"title": "DeBERTa XXL", "content": ""}, {"title": "GPT-2 medium/large", "content": ""}, {"title": "", "content": ""}, {"title": "Low-Rank-Parametrized Update Matrices", "content": ""}, {"title": "", "content": ""}, {"title": "Which Weight Matrices in Transformer Should We Apply LoRA to?", "content": ""}, {"title": "", "content": ""}, {"title": "RoBERTa", "content": ""}, {"title": "DeBERTa", "content": ""}, {"title": "GPT-2", "content": ""}, {"title": "", "content": ""}, {"title": "Additional Experiments on GPT-2", "content": ""}, {"title": "Additional Experiments on GPT-3", "content": ""}, {"title": "", "content": ""}, {"title": "Correlation between LoRA Modules", "content": ""}, {"title": "Effect of $r$ on GPT-2", "content": ""}, {"title": "Amplification Factor", "content": "{{(Left)}}\n{{(Center)}}\n{{(Right)}}\n{{(Top)}}\n{{(Bottom)}}\n{{(a)}}\n{{(b)}}\n{{(c)}}\n{{(d)}}\n[1]{{#1}}\n}\n}\n and }\n, ,  and }\n}\n}\n and }\n,  and }\n}\n}\n}\n}\n}\n--}\n}\n}\n and }\n and }\n}\n}\n and }\n}\n{}\n{}}}\n{}}}\n}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}\n}\n}\n}\n}\n}\n}\n}\n}\n}\n}\n}\n}\n}\n}\n}\n}\n}\n}\n}\n}\n}\n}\n}\n}\n}\n}\n}\n}\n}\n}\n}\n}\n}\n}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n{{{m}{sl}\n{bold}{{{bx}{n}\n[1]{}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}\n}\n}\n}\n}\n}\n}\n}\n}\n}\n}\n}\n}\n}\n}\n}\n}\n}\n}\n}\n}\n}\n}\n}\n}\n}\n}\n}\n[1]{}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n{p_{}}\n{_{}}\n{_{}}\n{p_{}}\n{P_{}}\n{_{}}\n{p_{}}\n{p_{}}\n{p_{}}\n{} %\n{}\n{}\n{}\n{}\n{\n{\n{}\n{}\n{\n{\n{D_{}}\n{}\n{}\n{}\n{L^0}\n{L^1}\n{L^2}\n{L^p}\n{L^\n{Pa} %\n{arg\n{arg\n{sign}\n{Tr}\n"}]}, {"title": "Conclusion and Future Work", "subsections": [{"title": "", "content": "\nWe evaluate the downstream task performance of LoRA on RoBERTa~, DeBERTa~, and GPT-2~, before scaling up to GPT-3 175B~.\nOur experiments cover a wide range of tasks, from natural language understanding (NLU) to generation (NLG).\nSpecifically, we evaluate on the GLUE~ benchmark for RoBERTa and DeBERTa.\nWe follow the setup of~ on GPT-2 for a direct comparison and add WikiSQL~ (NL to SQL queries) and SAMSum~ (conversation summarization) for large-scale experiments on GPT-3.\nSee  for more details on the datasets we use.\nWe use NVIDIA Tesla V100 for all experiments.\n"}, {"title": "Baselines", "content": "To compare with other baselines broadly, we replicate the setups used by prior work and reuse their reported numbers whenever possible.\nThis, however, means that some baselines might only appear in certain experiments.\nFine-Tuning (FT) is a common approach for adaptation.\nDuring fine-tuning, the model is initialized to the pre-trained weights and biases, and all model parameters undergo gradient updates.%\nA simple variant is to update only some layers while freezing others.\nWe include one such baseline reported in prior work~ on GPT-2, which adapts just the last two layers ($FT^{Top2}$).\nBias-only or BitFit is a baseline where we only train the bias vectors while freezing everything else.\nContemporarily, this baseline has also been studied by BitFit~.\nPrefix-embedding tuning (PreEmbed) inserts special tokens among the input tokens.\nThese special tokens have trainable word embeddings and are generally not in the model's vocabulary.\nWhere to place such tokens can have an impact on performance.\nWe focus on ``prefixing'', which prepends such tokens to the prompt, and ``infixing'', which appends to the prompt; both are discussed in~.\nWe use $l_{p}$ (resp. $l_{i}$) denote the number of prefix (resp. infix) tokens.\nThe number of trainable parameters is $|= d_{model} (l_p + l_i)$. \nPrefix-layer tuning (PreLayer) is an extension to prefix-embedding tuning.\nInstead of just learning the word embeddings (or equivalently, the activations after the embedding layer) for some special tokens, we learn the activations after every Transformer layer.\nThe activations computed from previous layers are simply replaced by trainable ones.\nThe resulting number of trainable parameters is $|= L d_{model} (l_p + l_i)$, where $L$ is the number of Transformer layers.\nAdapter tuning as proposed in~ inserts adapter layers between the self-attention module (and the MLP module) and the subsequent residual connection.\nThere are two fully connected layers with biases in an adapter layer with a nonlinearity in between.\nWe call this original design $Adapter^{H}$.\nRecently,  proposed a more efficient design with the adapter layer applied only after the MLP module and after a LayerNorm.\nWe call it $Adapter^{L}$.\nThis is very similar to another deign proposed in~, which we call $Adapter^{P}$.\nWe also include another baseline call AdapterDrop~ which drops some adapter layers for greater efficiency ($Adapter^{D}$).\nWe cite numbers from prior works whenever possible to maximize the number of baselines we compare with; they are in rows with an asterisk (*) in the first column.\nIn all cases,  we have $|= _{Adpt} (2 d_{model} r + r + d_{model}) + 2 _{LN} d_{model}$ where $_{Adpt}$ is the number of adapter layers and $_{LN}$ the number of trainable LayerNorms (e.g., in $^{}$).\nLoRA adds trainable pairs of rank decomposition matrices in parallel to existing weight matrices.\nAs mentioned in~, we only apply LoRA to $W_{q}$ and $W_{v}$ in most experiments for simplicity.\nThe number of trainable parameters is determined by the rank $r$ and the shape of the original weights: $|= 2 _{LoRA} d_{model} r$, where $_{LoRA}$ is the number of weight matrices we apply LoRA to.\n"}, {"title": "RoBERTa base/large", "content": ""}, {"title": "DeBERTa XXL", "content": ""}, {"title": "GPT-2 medium/large", "content": ""}, {"title": "", "content": ""}, {"title": "Low-Rank-Parametrized Update Matrices", "content": ""}, {"title": "", "content": ""}, {"title": "Which Weight Matrices in Transformer Should We Apply LoRA to?", "content": ""}, {"title": "", "content": ""}, {"title": "RoBERTa", "content": ""}, {"title": "DeBERTa", "content": ""}, {"title": "GPT-2", "content": ""}, {"title": "", "content": ""}, {"title": "Additional Experiments on GPT-2", "content": ""}, {"title": "Additional Experiments on GPT-3", "content": ""}, {"title": "", "content": ""}, {"title": "Correlation between LoRA Modules", "content": ""}, {"title": "Effect of $r$ on GPT-2", "content": ""}, {"title": "Amplification Factor", "content": "{{(Left)}}\n{{(Center)}}\n{{(Right)}}\n{{(Top)}}\n{{(Bottom)}}\n{{(a)}}\n{{(b)}}\n{{(c)}}\n{{(d)}}\n[1]{{#1}}\n}\n}\n and }\n, ,  and }\n}\n}\n and }\n,  and }\n}\n}\n}\n}\n}\n--}\n}\n}\n and }\n and }\n}\n}\n and }\n}\n{}\n{}}}\n{}}}\n}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}\n}\n}\n}\n}\n}\n}\n}\n}\n}\n}\n}\n}\n}\n}\n}\n}\n}\n}\n}\n}\n}\n}\n}\n}\n}\n}\n}\n}\n}\n}\n}\n}\n}\n}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n{{{m}{sl}\n{bold}{{{bx}{n}\n[1]{}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}\n}\n}\n}\n}\n}\n}\n}\n}\n}\n}\n}\n}\n}\n}\n}\n}\n}\n}\n}\n}\n}\n}\n}\n}\n}\n}\n}\n[1]{}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n{p_{}}\n{_{}}\n{_{}}\n{p_{}}\n{P_{}}\n{_{}}\n{p_{}}\n{p_{}}\n{p_{}}\n{} %\n{}\n{}\n{}\n{}\n{\n{\n{}\n{}\n{\n{\n{D_{}}\n{}\n{}\n{}\n{L^0}\n{L^1}\n{L^2}\n{L^p}\n{L^\n{Pa} %\n{arg\n{arg\n{sign}\n{Tr}\n"}]}, {"title": "Large Language Models Still Need Parameter Updates", "subsections": [{"title": "", "content": "\nWe evaluate the downstream task performance of LoRA on RoBERTa~, DeBERTa~, and GPT-2~, before scaling up to GPT-3 175B~.\nOur experiments cover a wide range of tasks, from natural language understanding (NLU) to generation (NLG).\nSpecifically, we evaluate on the GLUE~ benchmark for RoBERTa and DeBERTa.\nWe follow the setup of~ on GPT-2 for a direct comparison and add WikiSQL~ (NL to SQL queries) and SAMSum~ (conversation summarization) for large-scale experiments on GPT-3.\nSee  for more details on the datasets we use.\nWe use NVIDIA Tesla V100 for all experiments.\n"}, {"title": "Baselines", "content": "To compare with other baselines broadly, we replicate the setups used by prior work and reuse their reported numbers whenever possible.\nThis, however, means that some baselines might only appear in certain experiments.\nFine-Tuning (FT) is a common approach for adaptation.\nDuring fine-tuning, the model is initialized to the pre-trained weights and biases, and all model parameters undergo gradient updates.%\nA simple variant is to update only some layers while freezing others.\nWe include one such baseline reported in prior work~ on GPT-2, which adapts just the last two layers ($FT^{Top2}$).\nBias-only or BitFit is a baseline where we only train the bias vectors while freezing everything else.\nContemporarily, this baseline has also been studied by BitFit~.\nPrefix-embedding tuning (PreEmbed) inserts special tokens among the input tokens.\nThese special tokens have trainable word embeddings and are generally not in the model's vocabulary.\nWhere to place such tokens can have an impact on performance.\nWe focus on ``prefixing'', which prepends such tokens to the prompt, and ``infixing'', which appends to the prompt; both are discussed in~.\nWe use $l_{p}$ (resp. $l_{i}$) denote the number of prefix (resp. infix) tokens.\nThe number of trainable parameters is $|= d_{model} (l_p + l_i)$. \nPrefix-layer tuning (PreLayer) is an extension to prefix-embedding tuning.\nInstead of just learning the word embeddings (or equivalently, the activations after the embedding layer) for some special tokens, we learn the activations after every Transformer layer.\nThe activations computed from previous layers are simply replaced by trainable ones.\nThe resulting number of trainable parameters is $|= L d_{model} (l_p + l_i)$, where $L$ is the number of Transformer layers.\nAdapter tuning as proposed in~ inserts adapter layers between the self-attention module (and the MLP module) and the subsequent residual connection.\nThere are two fully connected layers with biases in an adapter layer with a nonlinearity in between.\nWe call this original design $Adapter^{H}$.\nRecently,  proposed a more efficient design with the adapter layer applied only after the MLP module and after a LayerNorm.\nWe call it $Adapter^{L}$.\nThis is very similar to another deign proposed in~, which we call $Adapter^{P}$.\nWe also include another baseline call AdapterDrop~ which drops some adapter layers for greater efficiency ($Adapter^{D}$).\nWe cite numbers from prior works whenever possible to maximize the number of baselines we compare with; they are in rows with an asterisk (*) in the first column.\nIn all cases,  we have $|= _{Adpt} (2 d_{model} r + r + d_{model}) + 2 _{LN} d_{model}$ where $_{Adpt}$ is the number of adapter layers and $_{LN}$ the number of trainable LayerNorms (e.g., in $^{}$).\nLoRA adds trainable pairs of rank decomposition matrices in parallel to existing weight matrices.\nAs mentioned in~, we only apply LoRA to $W_{q}$ and $W_{v}$ in most experiments for simplicity.\nThe number of trainable parameters is determined by the rank $r$ and the shape of the original weights: $|= 2 _{LoRA} d_{model} r$, where $_{LoRA}$ is the number of weight matrices we apply LoRA to.\n"}, {"title": "RoBERTa base/large", "content": ""}, {"title": "DeBERTa XXL", "content": ""}, {"title": "GPT-2 medium/large", "content": ""}, {"title": "", "content": ""}, {"title": "Low-Rank-Parametrized Update Matrices", "content": ""}, {"title": "", "content": ""}, {"title": "Which Weight Matrices in Transformer Should We Apply LoRA to?", "content": ""}, {"title": "", "content": ""}, {"title": "RoBERTa", "content": ""}, {"title": "DeBERTa", "content": ""}, {"title": "GPT-2", "content": ""}, {"title": "", "content": ""}, {"title": "Additional Experiments on GPT-2", "content": ""}, {"title": "Additional Experiments on GPT-3", "content": ""}, {"title": "", "content": ""}, {"title": "Correlation between LoRA Modules", "content": ""}, {"title": "Effect of $r$ on GPT-2", "content": ""}, {"title": "Amplification Factor", "content": "{{(Left)}}\n{{(Center)}}\n{{(Right)}}\n{{(Top)}}\n{{(Bottom)}}\n{{(a)}}\n{{(b)}}\n{{(c)}}\n{{(d)}}\n[1]{{#1}}\n}\n}\n and }\n, ,  and }\n}\n}\n and }\n,  and }\n}\n}\n}\n}\n}\n--}\n}\n}\n and }\n and }\n}\n}\n and }\n}\n{}\n{}}}\n{}}}\n}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}\n}\n}\n}\n}\n}\n}\n}\n}\n}\n}\n}\n}\n}\n}\n}\n}\n}\n}\n}\n}\n}\n}\n}\n}\n}\n}\n}\n}\n}\n}\n}\n}\n}\n}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n{{{m}{sl}\n{bold}{{{bx}{n}\n[1]{}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}\n}\n}\n}\n}\n}\n}\n}\n}\n}\n}\n}\n}\n}\n}\n}\n}\n}\n}\n}\n}\n}\n}\n}\n}\n}\n}\n}\n[1]{}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n{p_{}}\n{_{}}\n{_{}}\n{p_{}}\n{P_{}}\n{_{}}\n{p_{}}\n{p_{}}\n{p_{}}\n{} %\n{}\n{}\n{}\n{}\n{\n{\n{}\n{}\n{\n{\n{D_{}}\n{}\n{}\n{}\n{L^0}\n{L^1}\n{L^2}\n{L^p}\n{L^\n{Pa} %\n{arg\n{arg\n{sign}\n{Tr}\n"}]}, {"title": "Inference Latency Introduced by Adapter Layers", "subsections": [{"title": "", "content": "\nWe evaluate the downstream task performance of LoRA on RoBERTa~, DeBERTa~, and GPT-2~, before scaling up to GPT-3 175B~.\nOur experiments cover a wide range of tasks, from natural language understanding (NLU) to generation (NLG).\nSpecifically, we evaluate on the GLUE~ benchmark for RoBERTa and DeBERTa.\nWe follow the setup of~ on GPT-2 for a direct comparison and add WikiSQL~ (NL to SQL queries) and SAMSum~ (conversation summarization) for large-scale experiments on GPT-3.\nSee  for more details on the datasets we use.\nWe use NVIDIA Tesla V100 for all experiments.\n"}, {"title": "Baselines", "content": "To compare with other baselines broadly, we replicate the setups used by prior work and reuse their reported numbers whenever possible.\nThis, however, means that some baselines might only appear in certain experiments.\nFine-Tuning (FT) is a common approach for adaptation.\nDuring fine-tuning, the model is initialized to the pre-trained weights and biases, and all model parameters undergo gradient updates.%\nA simple variant is to update only some layers while freezing others.\nWe include one such baseline reported in prior work~ on GPT-2, which adapts just the last two layers ($FT^{Top2}$).\nBias-only or BitFit is a baseline where we only train the bias vectors while freezing everything else.\nContemporarily, this baseline has also been studied by BitFit~.\nPrefix-embedding tuning (PreEmbed) inserts special tokens among the input tokens.\nThese special tokens have trainable word embeddings and are generally not in the model's vocabulary.\nWhere to place such tokens can have an impact on performance.\nWe focus on ``prefixing'', which prepends such tokens to the prompt, and ``infixing'', which appends to the prompt; both are discussed in~.\nWe use $l_{p}$ (resp. $l_{i}$) denote the number of prefix (resp. infix) tokens.\nThe number of trainable parameters is $|= d_{model} (l_p + l_i)$. \nPrefix-layer tuning (PreLayer) is an extension to prefix-embedding tuning.\nInstead of just learning the word embeddings (or equivalently, the activations after the embedding layer) for some special tokens, we learn the activations after every Transformer layer.\nThe activations computed from previous layers are simply replaced by trainable ones.\nThe resulting number of trainable parameters is $|= L d_{model} (l_p + l_i)$, where $L$ is the number of Transformer layers.\nAdapter tuning as proposed in~ inserts adapter layers between the self-attention module (and the MLP module) and the subsequent residual connection.\nThere are two fully connected layers with biases in an adapter layer with a nonlinearity in between.\nWe call this original design $Adapter^{H}$.\nRecently,  proposed a more efficient design with the adapter layer applied only after the MLP module and after a LayerNorm.\nWe call it $Adapter^{L}$.\nThis is very similar to another deign proposed in~, which we call $Adapter^{P}$.\nWe also include another baseline call AdapterDrop~ which drops some adapter layers for greater efficiency ($Adapter^{D}$).\nWe cite numbers from prior works whenever possible to maximize the number of baselines we compare with; they are in rows with an asterisk (*) in the first column.\nIn all cases,  we have $|= _{Adpt} (2 d_{model} r + r + d_{model}) + 2 _{LN} d_{model}$ where $_{Adpt}$ is the number of adapter layers and $_{LN}$ the number of trainable LayerNorms (e.g., in $^{}$).\nLoRA adds trainable pairs of rank decomposition matrices in parallel to existing weight matrices.\nAs mentioned in~, we only apply LoRA to $W_{q}$ and $W_{v}$ in most experiments for simplicity.\nThe number of trainable parameters is determined by the rank $r$ and the shape of the original weights: $|= 2 _{LoRA} d_{model} r$, where $_{LoRA}$ is the number of weight matrices we apply LoRA to.\n"}, {"title": "RoBERTa base/large", "content": ""}, {"title": "DeBERTa XXL", "content": ""}, {"title": "GPT-2 medium/large", "content": ""}, {"title": "", "content": ""}, {"title": "Low-Rank-Parametrized Update Matrices", "content": ""}, {"title": "", "content": ""}, {"title": "Which Weight Matrices in Transformer Should We Apply LoRA to?", "content": ""}, {"title": "", "content": ""}, {"title": "RoBERTa", "content": ""}, {"title": "DeBERTa", "content": ""}, {"title": "GPT-2", "content": ""}, {"title": "", "content": ""}, {"title": "Additional Experiments on GPT-2", "content": ""}, {"title": "Additional Experiments on GPT-3", "content": ""}, {"title": "", "content": ""}, {"title": "Correlation between LoRA Modules", "content": ""}, {"title": "Effect of $r$ on GPT-2", "content": ""}, {"title": "Amplification Factor", "content": "{{(Left)}}\n{{(Center)}}\n{{(Right)}}\n{{(Top)}}\n{{(Bottom)}}\n{{(a)}}\n{{(b)}}\n{{(c)}}\n{{(d)}}\n[1]{{#1}}\n}\n}\n and }\n, ,  and }\n}\n}\n and }\n,  and }\n}\n}\n}\n}\n}\n--}\n}\n}\n and }\n and }\n}\n}\n and }\n}\n{}\n{}}}\n{}}}\n}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}\n}\n}\n}\n}\n}\n}\n}\n}\n}\n}\n}\n}\n}\n}\n}\n}\n}\n}\n}\n}\n}\n}\n}\n}\n}\n}\n}\n}\n}\n}\n}\n}\n}\n}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n{{{m}{sl}\n{bold}{{{bx}{n}\n[1]{}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}\n}\n}\n}\n}\n}\n}\n}\n}\n}\n}\n}\n}\n}\n}\n}\n}\n}\n}\n}\n}\n}\n}\n}\n}\n}\n}\n}\n[1]{}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n{p_{}}\n{_{}}\n{_{}}\n{p_{}}\n{P_{}}\n{_{}}\n{p_{}}\n{p_{}}\n{p_{}}\n{} %\n{}\n{}\n{}\n{}\n{\n{\n{}\n{}\n{\n{\n{D_{}}\n{}\n{}\n{}\n{L^0}\n{L^1}\n{L^2}\n{L^p}\n{L^\n{Pa} %\n{arg\n{arg\n{sign}\n{Tr}\n"}]}, {"title": "Dataset Details", "subsections": [{"title": "", "content": "\nWe evaluate the downstream task performance of LoRA on RoBERTa~, DeBERTa~, and GPT-2~, before scaling up to GPT-3 175B~.\nOur experiments cover a wide range of tasks, from natural language understanding (NLU) to generation (NLG).\nSpecifically, we evaluate on the GLUE~ benchmark for RoBERTa and DeBERTa.\nWe follow the setup of~ on GPT-2 for a direct comparison and add WikiSQL~ (NL to SQL queries) and SAMSum~ (conversation summarization) for large-scale experiments on GPT-3.\nSee  for more details on the datasets we use.\nWe use NVIDIA Tesla V100 for all experiments.\n"}, {"title": "Baselines", "content": "To compare with other baselines broadly, we replicate the setups used by prior work and reuse their reported numbers whenever possible.\nThis, however, means that some baselines might only appear in certain experiments.\nFine-Tuning (FT) is a common approach for adaptation.\nDuring fine-tuning, the model is initialized to the pre-trained weights and biases, and all model parameters undergo gradient updates.%\nA simple variant is to update only some layers while freezing others.\nWe include one such baseline reported in prior work~ on GPT-2, which adapts just the last two layers ($FT^{Top2}$).\nBias-only or BitFit is a baseline where we only train the bias vectors while freezing everything else.\nContemporarily, this baseline has also been studied by BitFit~.\nPrefix-embedding tuning (PreEmbed) inserts special tokens among the input tokens.\nThese special tokens have trainable word embeddings and are generally not in the model's vocabulary.\nWhere to place such tokens can have an impact on performance.\nWe focus on ``prefixing'', which prepends such tokens to the prompt, and ``infixing'', which appends to the prompt; both are discussed in~.\nWe use $l_{p}$ (resp. $l_{i}$) denote the number of prefix (resp. infix) tokens.\nThe number of trainable parameters is $|= d_{model} (l_p + l_i)$. \nPrefix-layer tuning (PreLayer) is an extension to prefix-embedding tuning.\nInstead of just learning the word embeddings (or equivalently, the activations after the embedding layer) for some special tokens, we learn the activations after every Transformer layer.\nThe activations computed from previous layers are simply replaced by trainable ones.\nThe resulting number of trainable parameters is $|= L d_{model} (l_p + l_i)$, where $L$ is the number of Transformer layers.\nAdapter tuning as proposed in~ inserts adapter layers between the self-attention module (and the MLP module) and the subsequent residual connection.\nThere are two fully connected layers with biases in an adapter layer with a nonlinearity in between.\nWe call this original design $Adapter^{H}$.\nRecently,  proposed a more efficient design with the adapter layer applied only after the MLP module and after a LayerNorm.\nWe call it $Adapter^{L}$.\nThis is very similar to another deign proposed in~, which we call $Adapter^{P}$.\nWe also include another baseline call AdapterDrop~ which drops some adapter layers for greater efficiency ($Adapter^{D}$).\nWe cite numbers from prior works whenever possible to maximize the number of baselines we compare with; they are in rows with an asterisk (*) in the first column.\nIn all cases,  we have $|= _{Adpt} (2 d_{model} r + r + d_{model}) + 2 _{LN} d_{model}$ where $_{Adpt}$ is the number of adapter layers and $_{LN}$ the number of trainable LayerNorms (e.g., in $^{}$).\nLoRA adds trainable pairs of rank decomposition matrices in parallel to existing weight matrices.\nAs mentioned in~, we only apply LoRA to $W_{q}$ and $W_{v}$ in most experiments for simplicity.\nThe number of trainable parameters is determined by the rank $r$ and the shape of the original weights: $|= 2 _{LoRA} d_{model} r$, where $_{LoRA}$ is the number of weight matrices we apply LoRA to.\n"}, {"title": "RoBERTa base/large", "content": ""}, {"title": "DeBERTa XXL", "content": ""}, {"title": "GPT-2 medium/large", "content": ""}, {"title": "", "content": ""}, {"title": "Low-Rank-Parametrized Update Matrices", "content": ""}, {"title": "", "content": ""}, {"title": "Which Weight Matrices in Transformer Should We Apply LoRA to?", "content": ""}, {"title": "", "content": ""}, {"title": "RoBERTa", "content": ""}, {"title": "DeBERTa", "content": ""}, {"title": "GPT-2", "content": ""}, {"title": "", "content": ""}, {"title": "Additional Experiments on GPT-2", "content": ""}, {"title": "Additional Experiments on GPT-3", "content": ""}, {"title": "", "content": ""}, {"title": "Correlation between LoRA Modules", "content": ""}, {"title": "Effect of $r$ on GPT-2", "content": ""}, {"title": "Amplification Factor", "content": "{{(Left)}}\n{{(Center)}}\n{{(Right)}}\n{{(Top)}}\n{{(Bottom)}}\n{{(a)}}\n{{(b)}}\n{{(c)}}\n{{(d)}}\n[1]{{#1}}\n}\n}\n and }\n, ,  and }\n}\n}\n and }\n,  and }\n}\n}\n}\n}\n}\n--}\n}\n}\n and }\n and }\n}\n}\n and }\n}\n{}\n{}}}\n{}}}\n}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}\n}\n}\n}\n}\n}\n}\n}\n}\n}\n}\n}\n}\n}\n}\n}\n}\n}\n}\n}\n}\n}\n}\n}\n}\n}\n}\n}\n}\n}\n}\n}\n}\n}\n}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n{{{m}{sl}\n{bold}{{{bx}{n}\n[1]{}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}\n}\n}\n}\n}\n}\n}\n}\n}\n}\n}\n}\n}\n}\n}\n}\n}\n}\n}\n}\n}\n}\n}\n}\n}\n}\n}\n}\n[1]{}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n{p_{}}\n{_{}}\n{_{}}\n{p_{}}\n{P_{}}\n{_{}}\n{p_{}}\n{p_{}}\n{p_{}}\n{} %\n{}\n{}\n{}\n{}\n{\n{\n{}\n{}\n{\n{\n{D_{}}\n{}\n{}\n{}\n{L^0}\n{L^1}\n{L^2}\n{L^p}\n{L^\n{Pa} %\n{arg\n{arg\n{sign}\n{Tr}\n"}]}, {"title": "Hyperparameters Used in Experiments", "subsections": [{"title": "", "content": "\nWe evaluate the downstream task performance of LoRA on RoBERTa~, DeBERTa~, and GPT-2~, before scaling up to GPT-3 175B~.\nOur experiments cover a wide range of tasks, from natural language understanding (NLU) to generation (NLG).\nSpecifically, we evaluate on the GLUE~ benchmark for RoBERTa and DeBERTa.\nWe follow the setup of~ on GPT-2 for a direct comparison and add WikiSQL~ (NL to SQL queries) and SAMSum~ (conversation summarization) for large-scale experiments on GPT-3.\nSee  for more details on the datasets we use.\nWe use NVIDIA Tesla V100 for all experiments.\n"}, {"title": "Baselines", "content": "To compare with other baselines broadly, we replicate the setups used by prior work and reuse their reported numbers whenever possible.\nThis, however, means that some baselines might only appear in certain experiments.\nFine-Tuning (FT) is a common approach for adaptation.\nDuring fine-tuning, the model is initialized to the pre-trained weights and biases, and all model parameters undergo gradient updates.%\nA simple variant is to update only some layers while freezing others.\nWe include one such baseline reported in prior work~ on GPT-2, which adapts just the last two layers ($FT^{Top2}$).\nBias-only or BitFit is a baseline where we only train the bias vectors while freezing everything else.\nContemporarily, this baseline has also been studied by BitFit~.\nPrefix-embedding tuning (PreEmbed) inserts special tokens among the input tokens.\nThese special tokens have trainable word embeddings and are generally not in the model's vocabulary.\nWhere to place such tokens can have an impact on performance.\nWe focus on ``prefixing'', which prepends such tokens to the prompt, and ``infixing'', which appends to the prompt; both are discussed in~.\nWe use $l_{p}$ (resp. $l_{i}$) denote the number of prefix (resp. infix) tokens.\nThe number of trainable parameters is $|= d_{model} (l_p + l_i)$. \nPrefix-layer tuning (PreLayer) is an extension to prefix-embedding tuning.\nInstead of just learning the word embeddings (or equivalently, the activations after the embedding layer) for some special tokens, we learn the activations after every Transformer layer.\nThe activations computed from previous layers are simply replaced by trainable ones.\nThe resulting number of trainable parameters is $|= L d_{model} (l_p + l_i)$, where $L$ is the number of Transformer layers.\nAdapter tuning as proposed in~ inserts adapter layers between the self-attention module (and the MLP module) and the subsequent residual connection.\nThere are two fully connected layers with biases in an adapter layer with a nonlinearity in between.\nWe call this original design $Adapter^{H}$.\nRecently,  proposed a more efficient design with the adapter layer applied only after the MLP module and after a LayerNorm.\nWe call it $Adapter^{L}$.\nThis is very similar to another deign proposed in~, which we call $Adapter^{P}$.\nWe also include another baseline call AdapterDrop~ which drops some adapter layers for greater efficiency ($Adapter^{D}$).\nWe cite numbers from prior works whenever possible to maximize the number of baselines we compare with; they are in rows with an asterisk (*) in the first column.\nIn all cases,  we have $|= _{Adpt} (2 d_{model} r + r + d_{model}) + 2 _{LN} d_{model}$ where $_{Adpt}$ is the number of adapter layers and $_{LN}$ the number of trainable LayerNorms (e.g., in $^{}$).\nLoRA adds trainable pairs of rank decomposition matrices in parallel to existing weight matrices.\nAs mentioned in~, we only apply LoRA to $W_{q}$ and $W_{v}$ in most experiments for simplicity.\nThe number of trainable parameters is determined by the rank $r$ and the shape of the original weights: $|= 2 _{LoRA} d_{model} r$, where $_{LoRA}$ is the number of weight matrices we apply LoRA to.\n"}, {"title": "RoBERTa base/large", "content": ""}, {"title": "DeBERTa XXL", "content": ""}, {"title": "GPT-2 medium/large", "content": ""}, {"title": "", "content": ""}, {"title": "Low-Rank-Parametrized Update Matrices", "content": ""}, {"title": "", "content": ""}, {"title": "Which Weight Matrices in Transformer Should We Apply LoRA to?", "content": ""}, {"title": "", "content": ""}, {"title": "RoBERTa", "content": ""}, {"title": "DeBERTa", "content": ""}, {"title": "GPT-2", "content": ""}, {"title": "", "content": ""}, {"title": "Additional Experiments on GPT-2", "content": ""}, {"title": "Additional Experiments on GPT-3", "content": ""}, {"title": "", "content": ""}, {"title": "Correlation between LoRA Modules", "content": ""}, {"title": "Effect of $r$ on GPT-2", "content": ""}, {"title": "Amplification Factor", "content": "{{(Left)}}\n{{(Center)}}\n{{(Right)}}\n{{(Top)}}\n{{(Bottom)}}\n{{(a)}}\n{{(b)}}\n{{(c)}}\n{{(d)}}\n[1]{{#1}}\n}\n}\n and }\n, ,  and }\n}\n}\n and }\n,  and }\n}\n}\n}\n}\n}\n--}\n}\n}\n and }\n and }\n}\n}\n and }\n}\n{}\n{}}}\n{}}}\n}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}\n}\n}\n}\n}\n}\n}\n}\n}\n}\n}\n}\n}\n}\n}\n}\n}\n}\n}\n}\n}\n}\n}\n}\n}\n}\n}\n}\n}\n}\n}\n}\n}\n}\n}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n{{{m}{sl}\n{bold}{{{bx}{n}\n[1]{}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}\n}\n}\n}\n}\n}\n}\n}\n}\n}\n}\n}\n}\n}\n}\n}\n}\n}\n}\n}\n}\n}\n}\n}\n}\n}\n}\n}\n[1]{}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n{p_{}}\n{_{}}\n{_{}}\n{p_{}}\n{P_{}}\n{_{}}\n{p_{}}\n{p_{}}\n{p_{}}\n{} %\n{}\n{}\n{}\n{}\n{\n{\n{}\n{}\n{\n{\n{D_{}}\n{}\n{}\n{}\n{L^0}\n{L^1}\n{L^2}\n{L^p}\n{L^\n{Pa} %\n{arg\n{arg\n{sign}\n{Tr}\n"}]}, {"title": "Combining LoRA with Prefix Tuning", "subsections": [{"title": "", "content": "\nWe evaluate the downstream task performance of LoRA on RoBERTa~, DeBERTa~, and GPT-2~, before scaling up to GPT-3 175B~.\nOur experiments cover a wide range of tasks, from natural language understanding (NLU) to generation (NLG).\nSpecifically, we evaluate on the GLUE~ benchmark for RoBERTa and DeBERTa.\nWe follow the setup of~ on GPT-2 for a direct comparison and add WikiSQL~ (NL to SQL queries) and SAMSum~ (conversation summarization) for large-scale experiments on GPT-3.\nSee  for more details on the datasets we use.\nWe use NVIDIA Tesla V100 for all experiments.\n"}, {"title": "Baselines", "content": "To compare with other baselines broadly, we replicate the setups used by prior work and reuse their reported numbers whenever possible.\nThis, however, means that some baselines might only appear in certain experiments.\nFine-Tuning (FT) is a common approach for adaptation.\nDuring fine-tuning, the model is initialized to the pre-trained weights and biases, and all model parameters undergo gradient updates.%\nA simple variant is to update only some layers while freezing others.\nWe include one such baseline reported in prior work~ on GPT-2, which adapts just the last two layers ($FT^{Top2}$).\nBias-only or BitFit is a baseline where we only train the bias vectors while freezing everything else.\nContemporarily, this baseline has also been studied by BitFit~.\nPrefix-embedding tuning (PreEmbed) inserts special tokens among the input tokens.\nThese special tokens have trainable word embeddings and are generally not in the model's vocabulary.\nWhere to place such tokens can have an impact on performance.\nWe focus on ``prefixing'', which prepends such tokens to the prompt, and ``infixing'', which appends to the prompt; both are discussed in~.\nWe use $l_{p}$ (resp. $l_{i}$) denote the number of prefix (resp. infix) tokens.\nThe number of trainable parameters is $|= d_{model} (l_p + l_i)$. \nPrefix-layer tuning (PreLayer) is an extension to prefix-embedding tuning.\nInstead of just learning the word embeddings (or equivalently, the activations after the embedding layer) for some special tokens, we learn the activations after every Transformer layer.\nThe activations computed from previous layers are simply replaced by trainable ones.\nThe resulting number of trainable parameters is $|= L d_{model} (l_p + l_i)$, where $L$ is the number of Transformer layers.\nAdapter tuning as proposed in~ inserts adapter layers between the self-attention module (and the MLP module) and the subsequent residual connection.\nThere are two fully connected layers with biases in an adapter layer with a nonlinearity in between.\nWe call this original design $Adapter^{H}$.\nRecently,  proposed a more efficient design with the adapter layer applied only after the MLP module and after a LayerNorm.\nWe call it $Adapter^{L}$.\nThis is very similar to another deign proposed in~, which we call $Adapter^{P}$.\nWe also include another baseline call AdapterDrop~ which drops some adapter layers for greater efficiency ($Adapter^{D}$).\nWe cite numbers from prior works whenever possible to maximize the number of baselines we compare with; they are in rows with an asterisk (*) in the first column.\nIn all cases,  we have $|= _{Adpt} (2 d_{model} r + r + d_{model}) + 2 _{LN} d_{model}$ where $_{Adpt}$ is the number of adapter layers and $_{LN}$ the number of trainable LayerNorms (e.g., in $^{}$).\nLoRA adds trainable pairs of rank decomposition matrices in parallel to existing weight matrices.\nAs mentioned in~, we only apply LoRA to $W_{q}$ and $W_{v}$ in most experiments for simplicity.\nThe number of trainable parameters is determined by the rank $r$ and the shape of the original weights: $|= 2 _{LoRA} d_{model} r$, where $_{LoRA}$ is the number of weight matrices we apply LoRA to.\n"}, {"title": "RoBERTa base/large", "content": ""}, {"title": "DeBERTa XXL", "content": ""}, {"title": "GPT-2 medium/large", "content": ""}, {"title": "", "content": ""}, {"title": "Low-Rank-Parametrized Update Matrices", "content": ""}, {"title": "", "content": ""}, {"title": "Which Weight Matrices in Transformer Should We Apply LoRA to?", "content": ""}, {"title": "", "content": ""}, {"title": "RoBERTa", "content": ""}, {"title": "DeBERTa", "content": ""}, {"title": "GPT-2", "content": ""}, {"title": "", "content": ""}, {"title": "Additional Experiments on GPT-2", "content": ""}, {"title": "Additional Experiments on GPT-3", "content": ""}, {"title": "", "content": ""}, {"title": "Correlation between LoRA Modules", "content": ""}, {"title": "Effect of $r$ on GPT-2", "content": ""}, {"title": "Amplification Factor", "content": "{{(Left)}}\n{{(Center)}}\n{{(Right)}}\n{{(Top)}}\n{{(Bottom)}}\n{{(a)}}\n{{(b)}}\n{{(c)}}\n{{(d)}}\n[1]{{#1}}\n}\n}\n and }\n, ,  and }\n}\n}\n and }\n,  and }\n}\n}\n}\n}\n}\n--}\n}\n}\n and }\n and }\n}\n}\n and }\n}\n{}\n{}}}\n{}}}\n}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}\n}\n}\n}\n}\n}\n}\n}\n}\n}\n}\n}\n}\n}\n}\n}\n}\n}\n}\n}\n}\n}\n}\n}\n}\n}\n}\n}\n}\n}\n}\n}\n}\n}\n}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n{{{m}{sl}\n{bold}{{{bx}{n}\n[1]{}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}\n}\n}\n}\n}\n}\n}\n}\n}\n}\n}\n}\n}\n}\n}\n}\n}\n}\n}\n}\n}\n}\n}\n}\n}\n}\n}\n}\n[1]{}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n{p_{}}\n{_{}}\n{_{}}\n{p_{}}\n{P_{}}\n{_{}}\n{p_{}}\n{p_{}}\n{p_{}}\n{} %\n{}\n{}\n{}\n{}\n{\n{\n{}\n{}\n{\n{\n{D_{}}\n{}\n{}\n{}\n{L^0}\n{L^1}\n{L^2}\n{L^p}\n{L^\n{Pa} %\n{arg\n{arg\n{sign}\n{Tr}\n"}]}, {"title": "Additional Empirical Experiments", "subsections": [{"title": "", "content": "\nWe evaluate the downstream task performance of LoRA on RoBERTa~, DeBERTa~, and GPT-2~, before scaling up to GPT-3 175B~.\nOur experiments cover a wide range of tasks, from natural language understanding (NLU) to generation (NLG).\nSpecifically, we evaluate on the GLUE~ benchmark for RoBERTa and DeBERTa.\nWe follow the setup of~ on GPT-2 for a direct comparison and add WikiSQL~ (NL to SQL queries) and SAMSum~ (conversation summarization) for large-scale experiments on GPT-3.\nSee  for more details on the datasets we use.\nWe use NVIDIA Tesla V100 for all experiments.\n"}, {"title": "Baselines", "content": "To compare with other baselines broadly, we replicate the setups used by prior work and reuse their reported numbers whenever possible.\nThis, however, means that some baselines might only appear in certain experiments.\nFine-Tuning (FT) is a common approach for adaptation.\nDuring fine-tuning, the model is initialized to the pre-trained weights and biases, and all model parameters undergo gradient updates.%\nA simple variant is to update only some layers while freezing others.\nWe include one such baseline reported in prior work~ on GPT-2, which adapts just the last two layers ($FT^{Top2}$).\nBias-only or BitFit is a baseline where we only train the bias vectors while freezing everything else.\nContemporarily, this baseline has also been studied by BitFit~.\nPrefix-embedding tuning (PreEmbed) inserts special tokens among the input tokens.\nThese special tokens have trainable word embeddings and are generally not in the model's vocabulary.\nWhere to place such tokens can have an impact on performance.\nWe focus on ``prefixing'', which prepends such tokens to the prompt, and ``infixing'', which appends to the prompt; both are discussed in~.\nWe use $l_{p}$ (resp. $l_{i}$) denote the number of prefix (resp. infix) tokens.\nThe number of trainable parameters is $|= d_{model} (l_p + l_i)$. \nPrefix-layer tuning (PreLayer) is an extension to prefix-embedding tuning.\nInstead of just learning the word embeddings (or equivalently, the activations after the embedding layer) for some special tokens, we learn the activations after every Transformer layer.\nThe activations computed from previous layers are simply replaced by trainable ones.\nThe resulting number of trainable parameters is $|= L d_{model} (l_p + l_i)$, where $L$ is the number of Transformer layers.\nAdapter tuning as proposed in~ inserts adapter layers between the self-attention module (and the MLP module) and the subsequent residual connection.\nThere are two fully connected layers with biases in an adapter layer with a nonlinearity in between.\nWe call this original design $Adapter^{H}$.\nRecently,  proposed a more efficient design with the adapter layer applied only after the MLP module and after a LayerNorm.\nWe call it $Adapter^{L}$.\nThis is very similar to another deign proposed in~, which we call $Adapter^{P}$.\nWe also include another baseline call AdapterDrop~ which drops some adapter layers for greater efficiency ($Adapter^{D}$).\nWe cite numbers from prior works whenever possible to maximize the number of baselines we compare with; they are in rows with an asterisk (*) in the first column.\nIn all cases,  we have $|= _{Adpt} (2 d_{model} r + r + d_{model}) + 2 _{LN} d_{model}$ where $_{Adpt}$ is the number of adapter layers and $_{LN}$ the number of trainable LayerNorms (e.g., in $^{}$).\nLoRA adds trainable pairs of rank decomposition matrices in parallel to existing weight matrices.\nAs mentioned in~, we only apply LoRA to $W_{q}$ and $W_{v}$ in most experiments for simplicity.\nThe number of trainable parameters is determined by the rank $r$ and the shape of the original weights: $|= 2 _{LoRA} d_{model} r$, where $_{LoRA}$ is the number of weight matrices we apply LoRA to.\n"}, {"title": "RoBERTa base/large", "content": ""}, {"title": "DeBERTa XXL", "content": ""}, {"title": "GPT-2 medium/large", "content": ""}, {"title": "", "content": ""}, {"title": "Low-Rank-Parametrized Update Matrices", "content": ""}, {"title": "", "content": ""}, {"title": "Which Weight Matrices in Transformer Should We Apply LoRA to?", "content": ""}, {"title": "", "content": ""}, {"title": "RoBERTa", "content": ""}, {"title": "DeBERTa", "content": ""}, {"title": "GPT-2", "content": ""}, {"title": "", "content": ""}, {"title": "Additional Experiments on GPT-2", "content": ""}, {"title": "Additional Experiments on GPT-3", "content": ""}, {"title": "", "content": ""}, {"title": "Correlation between LoRA Modules", "content": ""}, {"title": "Effect of $r$ on GPT-2", "content": ""}, {"title": "Amplification Factor", "content": "{{(Left)}}\n{{(Center)}}\n{{(Right)}}\n{{(Top)}}\n{{(Bottom)}}\n{{(a)}}\n{{(b)}}\n{{(c)}}\n{{(d)}}\n[1]{{#1}}\n}\n}\n and }\n, ,  and }\n}\n}\n and }\n,  and }\n}\n}\n}\n}\n}\n--}\n}\n}\n and }\n and }\n}\n}\n and }\n}\n{}\n{}}}\n{}}}\n}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}\n}\n}\n}\n}\n}\n}\n}\n}\n}\n}\n}\n}\n}\n}\n}\n}\n}\n}\n}\n}\n}\n}\n}\n}\n}\n}\n}\n}\n}\n}\n}\n}\n}\n}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n{{{m}{sl}\n{bold}{{{bx}{n}\n[1]{}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}\n}\n}\n}\n}\n}\n}\n}\n}\n}\n}\n}\n}\n}\n}\n}\n}\n}\n}\n}\n}\n}\n}\n}\n}\n}\n}\n}\n[1]{}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n{p_{}}\n{_{}}\n{_{}}\n{p_{}}\n{P_{}}\n{_{}}\n{p_{}}\n{p_{}}\n{p_{}}\n{} %\n{}\n{}\n{}\n{}\n{\n{\n{}\n{}\n{\n{\n{D_{}}\n{}\n{}\n{}\n{L^0}\n{L^1}\n{L^2}\n{L^p}\n{L^\n{Pa} %\n{arg\n{arg\n{sign}\n{Tr}\n"}]}, {"title": "Measuring Similarity Between Subspaces", "subsections": [{"title": "", "content": "\nWe evaluate the downstream task performance of LoRA on RoBERTa~, DeBERTa~, and GPT-2~, before scaling up to GPT-3 175B~.\nOur experiments cover a wide range of tasks, from natural language understanding (NLU) to generation (NLG).\nSpecifically, we evaluate on the GLUE~ benchmark for RoBERTa and DeBERTa.\nWe follow the setup of~ on GPT-2 for a direct comparison and add WikiSQL~ (NL to SQL queries) and SAMSum~ (conversation summarization) for large-scale experiments on GPT-3.\nSee  for more details on the datasets we use.\nWe use NVIDIA Tesla V100 for all experiments.\n"}, {"title": "Baselines", "content": "To compare with other baselines broadly, we replicate the setups used by prior work and reuse their reported numbers whenever possible.\nThis, however, means that some baselines might only appear in certain experiments.\nFine-Tuning (FT) is a common approach for adaptation.\nDuring fine-tuning, the model is initialized to the pre-trained weights and biases, and all model parameters undergo gradient updates.%\nA simple variant is to update only some layers while freezing others.\nWe include one such baseline reported in prior work~ on GPT-2, which adapts just the last two layers ($FT^{Top2}$).\nBias-only or BitFit is a baseline where we only train the bias vectors while freezing everything else.\nContemporarily, this baseline has also been studied by BitFit~.\nPrefix-embedding tuning (PreEmbed) inserts special tokens among the input tokens.\nThese special tokens have trainable word embeddings and are generally not in the model's vocabulary.\nWhere to place such tokens can have an impact on performance.\nWe focus on ``prefixing'', which prepends such tokens to the prompt, and ``infixing'', which appends to the prompt; both are discussed in~.\nWe use $l_{p}$ (resp. $l_{i}$) denote the number of prefix (resp. infix) tokens.\nThe number of trainable parameters is $|= d_{model} (l_p + l_i)$. \nPrefix-layer tuning (PreLayer) is an extension to prefix-embedding tuning.\nInstead of just learning the word embeddings (or equivalently, the activations after the embedding layer) for some special tokens, we learn the activations after every Transformer layer.\nThe activations computed from previous layers are simply replaced by trainable ones.\nThe resulting number of trainable parameters is $|= L d_{model} (l_p + l_i)$, where $L$ is the number of Transformer layers.\nAdapter tuning as proposed in~ inserts adapter layers between the self-attention module (and the MLP module) and the subsequent residual connection.\nThere are two fully connected layers with biases in an adapter layer with a nonlinearity in between.\nWe call this original design $Adapter^{H}$.\nRecently,  proposed a more efficient design with the adapter layer applied only after the MLP module and after a LayerNorm.\nWe call it $Adapter^{L}$.\nThis is very similar to another deign proposed in~, which we call $Adapter^{P}$.\nWe also include another baseline call AdapterDrop~ which drops some adapter layers for greater efficiency ($Adapter^{D}$).\nWe cite numbers from prior works whenever possible to maximize the number of baselines we compare with; they are in rows with an asterisk (*) in the first column.\nIn all cases,  we have $|= _{Adpt} (2 d_{model} r + r + d_{model}) + 2 _{LN} d_{model}$ where $_{Adpt}$ is the number of adapter layers and $_{LN}$ the number of trainable LayerNorms (e.g., in $^{}$).\nLoRA adds trainable pairs of rank decomposition matrices in parallel to existing weight matrices.\nAs mentioned in~, we only apply LoRA to $W_{q}$ and $W_{v}$ in most experiments for simplicity.\nThe number of trainable parameters is determined by the rank $r$ and the shape of the original weights: $|= 2 _{LoRA} d_{model} r$, where $_{LoRA}$ is the number of weight matrices we apply LoRA to.\n"}, {"title": "RoBERTa base/large", "content": ""}, {"title": "DeBERTa XXL", "content": ""}, {"title": "GPT-2 medium/large", "content": ""}, {"title": "", "content": ""}, {"title": "Low-Rank-Parametrized Update Matrices", "content": ""}, {"title": "", "content": ""}, {"title": "Which Weight Matrices in Transformer Should We Apply LoRA to?", "content": ""}, {"title": "", "content": ""}, {"title": "RoBERTa", "content": ""}, {"title": "DeBERTa", "content": ""}, {"title": "GPT-2", "content": ""}, {"title": "", "content": ""}, {"title": "Additional Experiments on GPT-2", "content": ""}, {"title": "Additional Experiments on GPT-3", "content": ""}, {"title": "", "content": ""}, {"title": "Correlation between LoRA Modules", "content": ""}, {"title": "Effect of $r$ on GPT-2", "content": ""}, {"title": "Amplification Factor", "content": "{{(Left)}}\n{{(Center)}}\n{{(Right)}}\n{{(Top)}}\n{{(Bottom)}}\n{{(a)}}\n{{(b)}}\n{{(c)}}\n{{(d)}}\n[1]{{#1}}\n}\n}\n and }\n, ,  and }\n}\n}\n and }\n,  and }\n}\n}\n}\n}\n}\n--}\n}\n}\n and }\n and }\n}\n}\n and }\n}\n{}\n{}}}\n{}}}\n}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}\n}\n}\n}\n}\n}\n}\n}\n}\n}\n}\n}\n}\n}\n}\n}\n}\n}\n}\n}\n}\n}\n}\n}\n}\n}\n}\n}\n}\n}\n}\n}\n}\n}\n}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n{{{m}{sl}\n{bold}{{{bx}{n}\n[1]{}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}\n}\n}\n}\n}\n}\n}\n}\n}\n}\n}\n}\n}\n}\n}\n}\n}\n}\n}\n}\n}\n}\n}\n}\n}\n}\n}\n}\n[1]{}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n{p_{}}\n{_{}}\n{_{}}\n{p_{}}\n{P_{}}\n{_{}}\n{p_{}}\n{p_{}}\n{p_{}}\n{} %\n{}\n{}\n{}\n{}\n{\n{\n{}\n{}\n{\n{\n{D_{}}\n{}\n{}\n{}\n{L^0}\n{L^1}\n{L^2}\n{L^p}\n{L^\n{Pa} %\n{arg\n{arg\n{sign}\n{Tr}\n"}]}, {"title": "Additional Experiments on Low-Rank Matrices", "subsections": [{"title": "", "content": "\nWe evaluate the downstream task performance of LoRA on RoBERTa~, DeBERTa~, and GPT-2~, before scaling up to GPT-3 175B~.\nOur experiments cover a wide range of tasks, from natural language understanding (NLU) to generation (NLG).\nSpecifically, we evaluate on the GLUE~ benchmark for RoBERTa and DeBERTa.\nWe follow the setup of~ on GPT-2 for a direct comparison and add WikiSQL~ (NL to SQL queries) and SAMSum~ (conversation summarization) for large-scale experiments on GPT-3.\nSee  for more details on the datasets we use.\nWe use NVIDIA Tesla V100 for all experiments.\n"}, {"title": "Baselines", "content": "To compare with other baselines broadly, we replicate the setups used by prior work and reuse their reported numbers whenever possible.\nThis, however, means that some baselines might only appear in certain experiments.\nFine-Tuning (FT) is a common approach for adaptation.\nDuring fine-tuning, the model is initialized to the pre-trained weights and biases, and all model parameters undergo gradient updates.%\nA simple variant is to update only some layers while freezing others.\nWe include one such baseline reported in prior work~ on GPT-2, which adapts just the last two layers ($FT^{Top2}$).\nBias-only or BitFit is a baseline where we only train the bias vectors while freezing everything else.\nContemporarily, this baseline has also been studied by BitFit~.\nPrefix-embedding tuning (PreEmbed) inserts special tokens among the input tokens.\nThese special tokens have trainable word embeddings and are generally not in the model's vocabulary.\nWhere to place such tokens can have an impact on performance.\nWe focus on ``prefixing'', which prepends such tokens to the prompt, and ``infixing'', which appends to the prompt; both are discussed in~.\nWe use $l_{p}$ (resp. $l_{i}$) denote the number of prefix (resp. infix) tokens.\nThe number of trainable parameters is $|= d_{model} (l_p + l_i)$. \nPrefix-layer tuning (PreLayer) is an extension to prefix-embedding tuning.\nInstead of just learning the word embeddings (or equivalently, the activations after the embedding layer) for some special tokens, we learn the activations after every Transformer layer.\nThe activations computed from previous layers are simply replaced by trainable ones.\nThe resulting number of trainable parameters is $|= L d_{model} (l_p + l_i)$, where $L$ is the number of Transformer layers.\nAdapter tuning as proposed in~ inserts adapter layers between the self-attention module (and the MLP module) and the subsequent residual connection.\nThere are two fully connected layers with biases in an adapter layer with a nonlinearity in between.\nWe call this original design $Adapter^{H}$.\nRecently,  proposed a more efficient design with the adapter layer applied only after the MLP module and after a LayerNorm.\nWe call it $Adapter^{L}$.\nThis is very similar to another deign proposed in~, which we call $Adapter^{P}$.\nWe also include another baseline call AdapterDrop~ which drops some adapter layers for greater efficiency ($Adapter^{D}$).\nWe cite numbers from prior works whenever possible to maximize the number of baselines we compare with; they are in rows with an asterisk (*) in the first column.\nIn all cases,  we have $|= _{Adpt} (2 d_{model} r + r + d_{model}) + 2 _{LN} d_{model}$ where $_{Adpt}$ is the number of adapter layers and $_{LN}$ the number of trainable LayerNorms (e.g., in $^{}$).\nLoRA adds trainable pairs of rank decomposition matrices in parallel to existing weight matrices.\nAs mentioned in~, we only apply LoRA to $W_{q}$ and $W_{v}$ in most experiments for simplicity.\nThe number of trainable parameters is determined by the rank $r$ and the shape of the original weights: $|= 2 _{LoRA} d_{model} r$, where $_{LoRA}$ is the number of weight matrices we apply LoRA to.\n"}, {"title": "RoBERTa base/large", "content": ""}, {"title": "DeBERTa XXL", "content": ""}, {"title": "GPT-2 medium/large", "content": ""}, {"title": "", "content": ""}, {"title": "Low-Rank-Parametrized Update Matrices", "content": ""}, {"title": "", "content": ""}, {"title": "Which Weight Matrices in Transformer Should We Apply LoRA to?", "content": ""}, {"title": "", "content": ""}, {"title": "RoBERTa", "content": ""}, {"title": "DeBERTa", "content": ""}, {"title": "GPT-2", "content": ""}, {"title": "", "content": ""}, {"title": "Additional Experiments on GPT-2", "content": ""}, {"title": "Additional Experiments on GPT-3", "content": ""}, {"title": "", "content": ""}, {"title": "Correlation between LoRA Modules", "content": ""}, {"title": "Effect of $r$ on GPT-2", "content": ""}, {"title": "Amplification Factor", "content": "{{(Left)}}\n{{(Center)}}\n{{(Right)}}\n{{(Top)}}\n{{(Bottom)}}\n{{(a)}}\n{{(b)}}\n{{(c)}}\n{{(d)}}\n[1]{{#1}}\n}\n}\n and }\n, ,  and }\n}\n}\n and }\n,  and }\n}\n}\n}\n}\n}\n--}\n}\n}\n and }\n and }\n}\n}\n and }\n}\n{}\n{}}}\n{}}}\n}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}\n}\n}\n}\n}\n}\n}\n}\n}\n}\n}\n}\n}\n}\n}\n}\n}\n}\n}\n}\n}\n}\n}\n}\n}\n}\n}\n}\n}\n}\n}\n}\n}\n}\n}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n{{{m}{sl}\n{bold}{{{bx}{n}\n[1]{}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}\n}\n}\n}\n}\n}\n}\n}\n}\n}\n}\n}\n}\n}\n}\n}\n}\n}\n}\n}\n}\n}\n}\n}\n}\n}\n}\n}\n[1]{}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n}}\n{p_{}}\n{_{}}\n{_{}}\n{p_{}}\n{P_{}}\n{_{}}\n{p_{}}\n{p_{}}\n{p_{}}\n{} %\n{}\n{}\n{}\n{}\n{\n{\n{}\n{}\n{\n{\n{D_{}}\n{}\n{}\n{}\n{L^0}\n{L^1}\n{L^2}\n{L^p}\n{L^\n{Pa} %\n{arg\n{arg\n{sign}\n{Tr}\n"}]}]